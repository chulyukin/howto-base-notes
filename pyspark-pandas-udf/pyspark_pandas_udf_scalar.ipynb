{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6b9979-d088-487d-9843-b54ccf2b6ac4",
   "metadata": {},
   "source": [
    "# PySpark - Pandas UDF. Скалярные типы функций (SCALAR, SCALAR_ITER) \n",
    "В данном Notebook рассматриваются типы функций: **SCALAR**, **SCALAR_ITER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c85e61-cead-4a69-95ed-d30ff87462af",
   "metadata": {},
   "source": [
    "| Тип | Скалярные/Групповые | Возвращаемое значение | Трансформация |\n",
    "|:----|:-----|:------|:--------------|\n",
    "| **SCALAR** | Скалярные | Series | Поэлементные преобразования (скалярные вычисления) |\n",
    "| **SCALAR_ITER** | Скалярные | Iterator[Series] | Батчевые преобразования (скалярные вычисления) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ab3e8-a4ed-488d-bebc-2d301774bc5d",
   "metadata": {},
   "source": [
    "### Описание  \n",
    "```python\n",
    "pyspark.sql.functions.pandas_udf (f=None, returnType=None, functionType=None)\n",
    "#    f=None,            - Функция для преобразования в UDF\n",
    "#    returnType=None,   - Тип возвращаемого значения\n",
    "#    functionType=None  - Тип UDF (depricted)\n",
    "```\n",
    "**Тип функций:** *SCALAR*, *SCALAR_ITER*\n",
    "\n",
    "### Параметр `returnType` (Строковые обозначения):\n",
    "```python\n",
    "# Примитивные типы\n",
    "@pandas_udf(\"int\")      # IntegerType\n",
    "@pandas_udf(\"long\")     # LongType  \n",
    "@pandas_udf(\"float\")    # FloatType\n",
    "@pandas_udf(\"double\")   # DoubleType\n",
    "@pandas_udf(\"string\")   # StringType\n",
    "@pandas_udf(\"boolean\")  # BooleanType\n",
    "@pandas_udf(\"date\")     # DateType\n",
    "@pandas_udf(\"timestamp\") # TimestampType\n",
    "\n",
    "# Сложные типы\n",
    "@pandas_udf(\"array<int>\")           # ArrayType(IntegerType())\n",
    "@pandas_udf(\"map<string,int>\")      # MapType(StringType(), IntegerType())\n",
    "@pandas_udf(\"struct<name:string,age:int>\") # StructType\n",
    "```\n",
    "\n",
    "Параметр, устанавливающий максимальное количество записей в одном **Arrow batch** при передаче данных между Spark и Python процессами:\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"<Количество строк в батче>\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40ee44df-8564-48a3-b62c-c50e6cdb3e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# БД для тестовых DataSet \n",
    "DATA_DB = \"pandas_udf_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a18c73-d947-4fe4-b15a-cefcb91390ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/26 01:53:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"python\")\n",
    "os.environ[\"SPARK_LOCAL_IP\"]='localhost'\n",
    "from pyspark import SparkContext, SparkConf#, HiveContext\n",
    "conf = SparkConf()\\\n",
    "             .setAppName(\"Example Spark\")\\\n",
    "             .setMaster(\"local[2]\")\\\n",
    "             .setAppName(\"CountingSheep\")\\\n",
    "             .set(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4330b6e6-aa4c-4f9d-89c2-85a63f76fa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.13.7 (main, Aug 20 2025 22:17:40)\n",
      "Spark context Web UI available at http://localhost:4040\n",
      "Spark context available as 'sc' (master = local[2], app id = local-1764111233941).\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b8d3e4-8bee-4b7b-98c2-59791ac98c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CountingSheep</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ec56c751400>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707fe64d-03fc-4d44-bcb9-b019892126fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f_\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import DataFrame, types\n",
    "from pyspark.sql.functions import col, udf, pandas_udf, rand\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d0dce-75df-4278-84c3-4560ca4f1a13",
   "metadata": {},
   "source": [
    "## Предварительная подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d738ce-403b-486b-b729-f29bbcf7e80d",
   "metadata": {},
   "source": [
    "### Тестовый DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e4c6c26-51c1-4415-9f5a-d58827293a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataSet\n",
    "dfData = spark.createDataFrame([\n",
    "  Row(rowid=1,  double_value1=1.75, double_value2=70.0,  str_value='Hello, World!!!',  date_value=date(2003, 6, 1), timestamp_value=datetime(2003, 1, 1, 12, 0)),\n",
    "  Row(rowid=2,  double_value1=1.80, double_value2=80.0,  str_value='Python@#$%^&*()',  date_value=date(2004, 6, 2), timestamp_value=datetime(2004, 1, 2, 15, 15)),\n",
    "  Row(rowid=3,  double_value1=1.65, double_value2=60.0,  str_value='Привет, мир!!! 123', date_value=date(2007, 5, 3), timestamp_value=datetime(2006, 1, 3, 19, 23)),\n",
    "  Row(rowid=4,  double_value1=1.60, double_value2=60.0,  str_value=None,       date_value=date(2007, 4, 10), timestamp_value=datetime(2007, 1, 7, 20, 44)),\n",
    "  Row(rowid=5,  double_value1=1.70, double_value2=90.0,  str_value='код (130) Номер 244=-55-56\")', date_value=date(2008, 4, 10), timestamp_value=datetime(2008, 1, 9, 21, 34)),    \n",
    "  Row(rowid=6,  double_value1=1.90, double_value2=90.0,  str_value=\"## Ключевые особенности 'SCALAR UDF:'\", date_value=date(2009, 4, 10), timestamp_value=datetime(2009, 4, 5, 23, 53)),\n",
    "  Row(rowid=7,  double_value1=1.97, double_value2=100.0, str_value=\"**Векто(р)и'\\зованные опе%%%рации**\", date_value=date(2010, 4, 10), timestamp_value=datetime(2010, 8, 23, 10, 30)), \n",
    "  Row(rowid=8,  double_value1=1.96, double_value2=110.0, str_value=\"?*;'';&()_\",   date_value=date(2011, 4, 10), timestamp_value=datetime(2011, 2, 5, 9, 19)), \n",
    "  Row(rowid=9,  double_value1=1.83, double_value2=71.0,  str_value=\"Simple String №1\",   date_value=date(2014, 5, 20), timestamp_value=datetime(2015, 12, 5, 19, 19)), \n",
    "  Row(rowid=10, double_value1=1.64, double_value2=87.0,  str_value=\"Simple String №2\",   date_value=date(2015, 5, 20), timestamp_value=datetime(2016, 1, 15, 19, 10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798cdf84-da0a-4b40-8dc9-fd28e28001f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rowid', 'bigint'),\n",
       " ('double_value1', 'double'),\n",
       " ('double_value2', 'double'),\n",
       " ('str_value', 'string'),\n",
       " ('date_value', 'date'),\n",
       " ('timestamp_value', 'timestamp')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fefc4bd6-6db7-4ba4-88a8-b4fbcfff7734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------------+--------------------+----------+-------------------+\n",
      "|rowid|double_value1|double_value2|           str_value|date_value|    timestamp_value|\n",
      "+-----+-------------+-------------+--------------------+----------+-------------------+\n",
      "|    1|         1.75|         70.0|     Hello, World!!!|2003-06-01|2003-01-01 12:00:00|\n",
      "|    2|          1.8|         80.0|     Python@#$%^&*()|2004-06-02|2004-01-02 15:15:00|\n",
      "|    3|         1.65|         60.0|  Привет, мир!!! 123|2007-05-03|2006-01-03 19:23:00|\n",
      "|    4|          1.6|         60.0|                NULL|2007-04-10|2007-01-07 20:44:00|\n",
      "|    5|          1.7|         90.0|код (130) Номер 2...|2008-04-10|2008-01-09 21:34:00|\n",
      "|    6|          1.9|         90.0|## Ключевые особе...|2009-04-10|2009-04-05 23:53:00|\n",
      "|    7|         1.97|        100.0|**Векто(р)и'\\зова...|2010-04-10|2010-08-23 10:30:00|\n",
      "|    8|         1.96|        110.0|          ?*;'';&()_|2011-04-10|2011-02-05 09:19:00|\n",
      "|    9|         1.83|         71.0|    Simple String №1|2014-05-20|2015-12-05 19:19:00|\n",
      "|   10|         1.64|         87.0|    Simple String №2|2015-05-20|2016-01-15 19:10:00|\n",
      "+-----+-------------+-------------+--------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfData.orderBy(\"rowid\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d154ce8-b961-4a5d-a968-f2667e1c0971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow batch size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Arrow batch size:\", spark.conf.get(\"spark.sql.execution.arrow.maxRecordsPerBatch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137f5ee-faf9-415a-a123-6a5d8500305c",
   "metadata": {},
   "source": [
    "## PANDAS_UDF. SCALAR Example\n",
    "\n",
    "**Особенности SCALAR UDF:**\n",
    "\n",
    "1. **Векторизованные операции** - работает с pandas Series, а не с отдельными значениями\n",
    "2. **Высокая производительность** - использует Apache Arrow для быстрой передачи данных\n",
    "3. **Простота использования** - знакомый pandas API\n",
    "4. **Type safety** - строгая типизация входных и выходных данных\n",
    "   \n",
    "**Пример определения:**\n",
    "```python\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "# устаревший синтаксис @pandas_udf(IntegerType(), PandasUDFType.SCALAR)\n",
    "@pandas_udf(\"int\")\n",
    "def slen(s):\n",
    "    return s.str.len()\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78639797-b671-49d1-ac0b-6029e42ba417",
   "metadata": {},
   "source": [
    "**SCALAR** используется, если:  \n",
    "- Данные помещаются в память\n",
    "- Простые преобразования без предыдущих состояний\n",
    "- Не требуется инициализация дорогих ресурсов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79a7d40c-9962-493b-ac9e-a8cde4b348c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyArrow version: 22.0.0\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "print(f\"PyArrow version: {pa.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78ccf9-07be-429e-b1c7-960e4dc862e1",
   "metadata": {},
   "source": [
    "### 1. Простая функция с одним входным параметром"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bc5a03c-80a4-454e-84ed-e0c1ec3ca617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+\n",
      "|rowid|value|squared_value|\n",
      "+-----+-----+-------------+\n",
      "|    1| 1.75|       3.0625|\n",
      "|    2|  1.8|         3.24|\n",
      "|    3| 1.65|       2.7225|\n",
      "|    4|  1.6|         2.56|\n",
      "|    5|  1.7|         2.89|\n",
      "|    6|  1.9|         3.61|\n",
      "|    7| 1.97|       3.8809|\n",
      "|    8| 1.96|       3.8416|\n",
      "|    9| 1.83|       3.3489|\n",
      "|   10| 1.64|       2.6896|\n",
      "+-----+-----+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Возведение значения в степень (квадрат)\n",
    "@pandas_udf(\"double\")\n",
    "def square_value(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Вычисляет квадрат числа\"\"\"\n",
    "    return (series ** 2).round(6)\n",
    "# Применение\n",
    "result = dfData.select(\"rowid\", col(\"double_value1\").alias(\"value\"), square_value(col(\"double_value1\")).alias(\"squared_value\"))\n",
    "result.orderBy(\"rowid\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d3d7c-2801-4189-a232-2c0874d30fc4",
   "metadata": {},
   "source": [
    "### 2. Pandas UDF с несколькими входными параметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f77b2ef-afcf-4cbe-8656-ba6cfd9c6904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+-----+\n",
      "|rowid|height|weight|  bmi|\n",
      "+-----+------+------+-----+\n",
      "|    1|  1.75|  70.0|22.86|\n",
      "|    2|   1.8|  80.0|24.69|\n",
      "|    3|  1.65|  60.0|22.04|\n",
      "|    4|   1.6|  60.0|23.44|\n",
      "|    5|   1.7|  90.0|31.14|\n",
      "|    6|   1.9|  90.0|24.93|\n",
      "|    7|  1.97| 100.0|25.77|\n",
      "|    8|  1.96| 110.0|28.63|\n",
      "|    9|  1.83|  71.0| 21.2|\n",
      "|   10|  1.64|  87.0|32.35|\n",
      "+-----+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def calculate_bmi(height: pd.Series, weight: pd.Series) -> pd.Series:\n",
    "    \"\"\"Вычисляет индекс массы тела\"\"\"\n",
    "    return (weight / (height ** 2)).round(2)\n",
    "\n",
    "# Применение\n",
    "result = dfData.select(\n",
    "    \"rowid\", col(\"double_value1\").alias(\"height\"), col(\"double_value2\").alias(\"weight\"),\n",
    "    calculate_bmi(col(\"height\"), col(\"weight\")).alias(\"bmi\")\n",
    ")\n",
    "result.orderBy(\"rowid\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d0a34-9dbb-49bb-adc9-bee87d015346",
   "metadata": {},
   "source": [
    "### 3. UDF со строковыми преобразованиями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bc28bfe-4fe7-4ffc-bbf5-a7eaa769c727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------+-------------------------------+\n",
      "|rowid|str_value                            |cleaned_value                  |\n",
      "+-----+-------------------------------------+-------------------------------+\n",
      "|1    |Hello, World!!!                      |Hello World                    |\n",
      "|2    |Python@#$%^&*()                      |Python                         |\n",
      "|3    |Привет, мир!!! 123                   |Привет мир 123                 |\n",
      "|4    |NULL                                 |NULL                           |\n",
      "|5    |код (130) Номер 244=-55-56\")         |код 130 Номер 2445556          |\n",
      "|6    |## Ключевые особенности 'SCALAR UDF:'|Ключевые особенности SCALAR UDF|\n",
      "|7    |**Векто(р)и'\\зованные опе%%%рации**  |Векторизованные операции       |\n",
      "|8    |?*;'';&()_                           |                               |\n",
      "|9    |Simple String №1                     |Simple String 1                |\n",
      "|10   |Simple String №2                     |Simple String 2                |\n",
      "+-----+-------------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Тестовые данные с \"грязным\" текстом\n",
    "@pandas_udf(\"string\")\n",
    "def clean_text(text_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Очистка текста от специальных символов\"\"\"\n",
    "    def clean_string(text):\n",
    "        if text is None:\n",
    "            return None\n",
    "        # Удаляем все кроме букв, цифр и пробелов\n",
    "        return re.sub(r'[^a-zA-Zа-яА-Я0-9\\s]', '', str(text)).strip()\n",
    "    return text_series.apply(clean_string)\n",
    "\n",
    "result = dfData.select(\"rowid\", \"str_value\", clean_text(col(\"str_value\")).alias(\"cleaned_value\"))\n",
    "result.orderBy(\"rowid\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2fd7d8-be65-42e4-8617-adad1acdfa84",
   "metadata": {},
   "source": [
    "### 4. Математические вычисления\n",
    "\n",
    "**Сложные математические вычисления** - сложная нелинейная функция\n",
    "```\n",
    "f(x) = log(x + 1) × √x + sin(x)\n",
    "\n",
    "где:\n",
    "ln — натуральный логарифм (основание e)\n",
    "√x — квадратный корень от x\n",
    "sin(x) — синус от x (в радианах)\n",
    "```\n",
    "\n",
    "**Z-score нормализация** — метод масштабирования данных, который преобразует значения так, чтобы они имели среднее значение 0 и стандартное отклонение 1.\n",
    "```\n",
    "Z = (X - μ) / σ\n",
    "\n",
    "где:\n",
    "Z = нормализованное значение (z-score)\n",
    "X = исходное значение\n",
    "μ = среднее значение выборки\n",
    "σ = стандартное отклонение выборки\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "485c793f-ee9a-4665-bd68-d67aa0a9fb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+--------------------+\n",
      "|rowid|             value|    complex_result|          normalized|\n",
      "+-----+------------------+------------------+--------------------+\n",
      "|    1|10.663677245877121| 7.076278235298124| -1.2240494902983483|\n",
      "|    2| 19.60486931373665|14.081761492402679| -0.6708086298707555|\n",
      "|    3| 29.66178526445094| 17.65941347704602|-0.04853178002861...|\n",
      "|    4| 41.56500955006113|23.520640946921596|  0.6879863481643397|\n",
      "|    5| 50.73531233369813|  28.5605906769476|  1.2554035520333788|\n",
      "|    6|  60.3544999140733|31.365179174605387| -1.2355627443590032|\n",
      "|    7| 68.84843359065881| 34.97042746837918|  -0.683202185499726|\n",
      "|    8| 79.73964603043659| 38.28041232910466|0.025053425969692986|\n",
      "|    9| 88.89726635828859| 43.21905741184908|  0.6205735318138446|\n",
      "|   10|  98.9320868869754|44.798782779304176|   1.273137972075187|\n",
      "+-----+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def complex_calculation(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Сложные математические вычисления\"\"\"\n",
    "    return np.log(series + 1) * np.sqrt(series) + np.sin(series)\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def normalize_zscore(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Z-score нормализация\"\"\"\n",
    "    mean_val = series.mean()\n",
    "    std_val = series.std()\n",
    "    return (series - mean_val) / std_val\n",
    "\n",
    "# Применение\n",
    "data = [(i, float(i * 10 + np.random.randn())) for i in range(1, 11)]\n",
    "dfComplex = spark.createDataFrame(data, [\"rowid\", \"value\"])\n",
    "\n",
    "result = dfComplex.select(\n",
    "    \"rowid\", \"value\",\n",
    "    complex_calculation(col(\"value\")).alias(\"complex_result\"),\n",
    "    normalize_zscore(col(\"value\")).alias(\"normalized\")\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf65004-2c5e-4655-a410-d37df5df56b9",
   "metadata": {},
   "source": [
    "### 5. Обработка дат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4786a39-1337-4267-b349-63fdcc44d767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+----------------+\n",
      "|rowid|date_value|formatted_date      |days_since_epoch|\n",
      "+-----+----------+--------------------+----------------+\n",
      "|1    |2003-06-01|2003-06-01 Sunday   |12204           |\n",
      "|2    |2004-06-02|2004-06-02 Wednesday|12571           |\n",
      "|3    |2007-05-03|2007-05-03 Thursday |13636           |\n",
      "|4    |2007-04-10|2007-04-10 Tuesday  |13613           |\n",
      "|5    |2008-04-10|2008-04-10 Thursday |13979           |\n",
      "|6    |2009-04-10|2009-04-10 Friday   |14344           |\n",
      "|7    |2010-04-10|2010-04-10 Saturday |14709           |\n",
      "|8    |2011-04-10|2011-04-10 Sunday   |15074           |\n",
      "|9    |2014-05-20|2014-05-20 Tuesday  |16210           |\n",
      "|10   |2015-05-20|2015-05-20 Wednesday|16575           |\n",
      "+-----+----------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def format_date(date_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Форматирование даты\"\"\"\n",
    "    return pd.to_datetime(date_series).dt.strftime('%Y-%m-%d %A')\n",
    "\n",
    "@pandas_udf(\"int\")\n",
    "def days_since_epoch(date_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Количество дней с начала эпохи\"\"\"\n",
    "    epoch = pd.Timestamp('1970-01-01')\n",
    "    return (pd.to_datetime(date_series) - epoch).dt.days\n",
    "\n",
    "# Тестовые данные\n",
    "from datetime import date, timedelta\n",
    "dates_data = [\n",
    "    (1, date.today()),\n",
    "    (2, date.today() - timedelta(days=30)),\n",
    "    (3, date.today() + timedelta(days=15))\n",
    "]\n",
    "\n",
    "result = dfData.select(\n",
    "    \"rowid\", \"date_value\",\n",
    "    format_date(col(\"date_value\")).alias(\"formatted_date\"),\n",
    "    days_since_epoch(col(\"date_value\")).alias(\"days_since_epoch\")\n",
    ")\n",
    "result.orderBy(\"rowid\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8606bb-cba6-4a53-8631-19ae5b45daef",
   "metadata": {},
   "source": [
    "### 6. Условная логика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f48283de-01c2-4020-876a-7eff40bc389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"string\")\n",
    "def categorize_value(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Категоризация значений\"\"\"\n",
    "    def categorize(value):\n",
    "        if value < 10:\n",
    "            return \"Low\"\n",
    "        elif value < 50:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"High\"\n",
    "    \n",
    "    return series.apply(categorize)\n",
    "\n",
    "@pandas_udf(\"boolean\")\n",
    "def is_outlier(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Определение выбросов (значения вне 2 стандартных отклонений)\"\"\"\n",
    "    mean_val = series.mean()\n",
    "    std_val = series.std()\n",
    "    return (series - mean_val).abs() > 2 * std_val\n",
    "\n",
    "# Применение\n",
    "data = [(i, float(i * 5 + np.random.randn() * 10)) for i in range(1, 21)]\n",
    "dfLogic = spark.createDataFrame(data, [\"id\", \"value\"])\n",
    "\n",
    "result = dfLogic.select(\n",
    "                \"id\", \"value\",\n",
    "                 categorize_value(col(\"value\")).alias(\"category\"),\n",
    "                 is_outlier(col(\"value\")).alias(\"is_outlier\")\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05ef2188-efc4-4d76-8875-e418c4a20adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------+----------+\n",
      "| id|             value|category|is_outlier|\n",
      "+---+------------------+--------+----------+\n",
      "|  1|14.598566930062413|  Medium|     false|\n",
      "|  2| 18.09290069910137|  Medium|     false|\n",
      "|  3| 9.046838823206393|     Low|     false|\n",
      "|  4|20.161580902334457|  Medium|     false|\n",
      "|  5|28.221032477005032|  Medium|     false|\n",
      "|  6|39.477603580964114|  Medium|     false|\n",
      "|  7| 44.25479347454431|  Medium|     false|\n",
      "|  8| 49.17795107489015|  Medium|     false|\n",
      "|  9| 56.57539048925414|    High|     false|\n",
      "| 10|47.726490648508225|  Medium|     false|\n",
      "| 11| 60.25029361948212|    High|     false|\n",
      "| 12| 68.22748336391605|    High|     false|\n",
      "| 13| 47.33976697164026|  Medium|     false|\n",
      "| 14|59.307177481097376|    High|     false|\n",
      "| 15| 81.34310309234621|    High|     false|\n",
      "| 16| 85.16523399004258|    High|     false|\n",
      "| 17| 63.27064366793742|    High|     false|\n",
      "| 18|  95.9583632678565|    High|     false|\n",
      "| 19| 89.72266209231665|    High|     false|\n",
      "| 20|100.63573611935142|    High|     false|\n",
      "+---+------------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc8074-0426-48f7-83f6-e0cfd16f0899",
   "metadata": {},
   "source": [
    "### 7. Сравнение производительности UDF - Pandas UDF\n",
    "Произодится генерация 500000 строк, к которым применяется последовательно UDF и Pandas UDF, содержащие идентичное преобразование (возведение числа в квадрат )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f7b2389-4b45-48e2-8ca1-84f88c300ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обычная UDF (медленная)\n",
    "@udf(returnType=DoubleType())\n",
    "def slow_square(value):\n",
    "    return float(value ** 2)\n",
    "\n",
    "# Pandas UDF (быстрая)\n",
    "@pandas_udf(\"double\")\n",
    "def fast_square(series: pd.Series) -> pd.Series:\n",
    "    return series ** 2\n",
    "\n",
    "# Большой датасет для тестирования\n",
    "large_data = [(i, float(i)) for i in range(500000)]\n",
    "large_df = spark.createDataFrame(large_data, [\"id\", \"value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dd5fea-16e6-483a-ad66-c4d1e0d4861f",
   "metadata": {},
   "source": [
    "**Сравнение производительности**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fd0f6fd-018d-4ea5-9ef1-7b1aaf8771ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas UDF time: 0.46 seconds\n",
      "UDF time: 0.67 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Pandas UDF\n",
    "start = time.time()\n",
    "result_pandas = large_df.select(\"id\", fast_square(col(\"value\")).alias(\"squared\"))\n",
    "result_pandas.count()  # Trigger execution\n",
    "pandas_time = time.time() - start\n",
    "print(f\"Pandas UDF time: {pandas_time:.2f} seconds\")\n",
    "\n",
    "result_pandas = large_df.select(\"id\", slow_square(col(\"value\")).alias(\"squared\"))\n",
    "result_pandas.count()  # Trigger execution\n",
    "pandas_time = time.time() - start\n",
    "print(f\"UDF time: {pandas_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6c464-a3fa-4311-9379-cfb58c47b5a3",
   "metadata": {},
   "source": [
    "## PANDAS_UDF.SCALAR_ITER Example\n",
    "\n",
    "**Особенности SCALAR_ITER UDF (в дополнение к особенностям SCALAR):**\n",
    "\n",
    "1. **Батчевая обработка данных** - Данные поступают **частями**, а не все сразу\n",
    "2. **Стабильность** - Размер батча контролируется Spark (обычно тысячи записей)\n",
    "3. **Масштабируемость** - Позволяет обрабатывать **очень большие датасеты**\n",
    "4. **Управление памятью** - В памяти одновременно только один батч\n",
    "5. **Накопительные вычисления** - Использует накопленную информацию (возможность кэширования)\n",
    "6. **Потоковые агрегации** - Можно производить вычисления по окну\n",
    "7. **Обработка ошибок** -  Можно обрабатывать ошибки на уровне батчей\n",
    "\n",
    "**Преимущества SCALAR_ITER:**\n",
    "\n",
    "- **Контроль памяти** - обрабатывает данные по частям\n",
    "- **Эффективность** - можно переиспользовать дорогие объекты\n",
    "- **Гибкость** - позволяет сохранять состояние между батчами\n",
    "\n",
    "SCALAR_ITER особенно полезен для обработки больших объемов данных с ограниченной памятью"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2126400-21bb-4a0c-880e-43c0e2a45801",
   "metadata": {},
   "source": [
    "**Тестовые данные**\n",
    "Создается DataFrame 4000 строк - 5 партиций (~800 строк)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9d0378c-481b-4fb9-b89a-9cb1563611f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleData = [(i, float(i)) for i in range(1, 4001)]\n",
    "dfSample = spark.createDataFrame(sampleData, [\"rowid\", \"value\"]).repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb18036c-ba8f-441f-b17e-e193ad5dda6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры partitions: [801, 800, 799, 799, 801]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размеры partitions: {dfSample.rdd.glom().map(len).collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c06da-330a-4868-8acf-86acca09057d",
   "metadata": {},
   "source": [
    "**Установить размер батча**  \n",
    "```spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"250\")```\n",
    "\n",
    "✅ **Увеличить batch_size если используются:**\n",
    "- Простые операции (арифметические, простые функции)\n",
    "- Много доступной памяти\n",
    "- Нужна максимальная производительность\n",
    "- Большие датасеты\n",
    "\n",
    "⬇️ **Уменьшить batch_size если используются:**\n",
    "- Сложные операции (ML, тяжелые вычисления)\n",
    "- Ограниченная память\n",
    "- Нестабильные данные (много ошибок)\n",
    "- Потоковая обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1943d14-8080-4d38-acb2-f85cdca3cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"650\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c977026-408c-406d-9f31-976f7b1184fb",
   "metadata": {},
   "source": [
    "### 1. Базовый пример SCALAR_ITER\n",
    "Представлен общий принцип организации **pandas_udf** с батчевой обработкой.  \n",
    " - Создаются две функции: **SCALAR** и **SCALAR_ITER**  \n",
    " - Последовательно обрабатывается DataFrame **dfSample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dac14107-5bd3-402f-867c-dafcfa39d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "# SCALAR - работает с отдельными Series\n",
    "@pandas_udf(\"double\")\n",
    "def scalar_udf(series: pd.Series) -> pd.Series:\n",
    "    print(f\"SCALAR.Обрабатывается series {len(series)} строк\")\n",
    "    return series ** 2\n",
    "\n",
    "# SCALAR_ITER Pandas UDF с вызовом итератора в теле функции\n",
    "@pandas_udf(\"double\")\n",
    "def scalar_iter_udf(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Обрабатывает данные батчами для экономии памяти\"\"\"\n",
    "    batch_count = 0\n",
    "    total_rows = 0\n",
    "    for  i, batch in enumerate(iterator):\n",
    "        # Обработка каждого батча\n",
    "        total_rows += len(batch)\n",
    "        batch_count += 1\n",
    "        yield batch ** 2\n",
    "    print(f\"SCALAR_ITER.Обрабатывается партиция {total_rows} строк в {batch_count} батчах\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7ac7c-69ff-4a35-add7-c46169ce1e98",
   "metadata": {},
   "source": [
    "✅ **SCALAR UDF** обрабатывает series с максимальным количеством строк, равным **maxRecordsPerBatch**  \n",
    "(или размеру партиции в случае, если количество строк в партиции меньше maxRecordsPerBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "542b7e36-e5e0-4dd3-bbfa-9867296f9f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.88 ms, sys: 996 μs, total: 10.9 ms\n",
      "Wall time: 303 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCALAR.Обрабатывается series 650 строкSCALAR.Обрабатывается series 650 строк\n",
      "\n",
      "SCALAR.Обрабатывается series 151 строкSCALAR.Обрабатывается series 150 строк\n",
      "\n",
      "SCALAR.Обрабатывается series 650 строк\n",
      "SCALAR.Обрабатывается series 149 строк\n",
      "SCALAR.Обрабатывается series 650 строк\n",
      "SCALAR.Обрабатывается series 149 строк\n",
      "SCALAR.Обрабатывается series 650 строк\n",
      "SCALAR.Обрабатывается series 151 строк\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = dfSample.select(\"rowid\", scalar_udf(col(\"value\")).alias(\"squared\"))\n",
    "ret = result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20aa4c-f135-4ecc-af2d-6e57668ba55c",
   "metadata": {},
   "source": [
    "✅ **SCALAR_ITER UDF** обрабатывает каждую партицию батчами максимальным количеством строк, равным **maxRecordsPerBatch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "804da160-7af4-4f4e-906e-f163259dea57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 ms, sys: 1.93 ms, total: 13.3 ms\n",
      "Wall time: 278 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCALAR_ITER.Обрабатывается партиция 800 строк в 2 батчах\n",
      "SCALAR_ITER.Обрабатывается партиция 801 строк в 2 батчах\n",
      "SCALAR_ITER.Обрабатывается партиция 799 строк в 2 батчах\n",
      "SCALAR_ITER.Обрабатывается партиция 799 строк в 2 батчах\n",
      "SCALAR_ITER.Обрабатывается партиция 801 строк в 2 батчах\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = dfSample.select(\"rowid\", scalar_iter_udf(col(\"value\")).alias(\"squared\"))\n",
    "ret = result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959365e4-3858-49db-89cd-0ff1caef2a7f",
   "metadata": {},
   "source": [
    "### 2. Обработка данных с состоянием между батчами\n",
    "**Состояние между батчами** — это возможность сохранять и накапливать данные при обработке нескольких батчей данных. Позволяет создавать сложные алгоритмы потоковой обработки, которые \"помнят\" информацию из предыдущих порций данных.\n",
    "\n",
    "**Основной принцип:**\n",
    "```python\n",
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def stateful_processing(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "   # Состояние инициализируется один раз для всей partition\n",
    "   state_value = 0  # Состояние сохраняется между батчами\n",
    "    \n",
    "   for batch in iterator:  # Каждый batch обрабатывается последовательно\n",
    "       # Обновляется состояние переменной\n",
    "       state_value = <какой-то код>\n",
    "        \n",
    "       # Результат с учетом состояния\n",
    "       yield batch + state_value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589e2b8-d977-4aa7-ac77-7df11d568c89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Состояние между батчами** \n",
    "\n",
    "✅ **Сохраняется:**\n",
    "- Переменные, объявленные вне цикла `for batch in iterator`\n",
    "- Коллекции (списки, множества, словари)\n",
    "- Счетчики и аккумуляторы\n",
    "- Буферы для окон данных\n",
    "\n",
    "❌ **НЕ сохраняется:**\n",
    "- Состояние между разными partition\n",
    "- Состояние между разными executor'ами\n",
    "- Переменные внутри цикла батчей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba1c59-cb9e-455b-aa5e-86fc6122a729",
   "metadata": {},
   "source": [
    "**Сравнение со SCALAR UDF:**\n",
    "```python\n",
    "# ❌ SCALAR - состояние НЕ сохраняется\n",
    "@pandas_udf(\"double\")\n",
    "def scalar_no_state(series: pd.Series) -> pd.Series:\n",
    "    counter = 0  # Сбрасывается при каждом вызове\n",
    "    return series.cumsum()  # Работает только внутри series\n",
    "\n",
    "# ✅ SCALAR_ITER - состояние сохраняется\n",
    "@pandas_udf(\"double\")\n",
    "def scalar_iter_with_state(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    counter = 0  # Сохраняется между батчами\n",
    "    \n",
    "    for batch in iterator:\n",
    "        counter += len(batch)\n",
    "        yield batch + counter  # Учитывает все предыдущие батчи\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e671e6ea-616d-48e4-a426-bdfaa8d20357",
   "metadata": {},
   "source": [
    "#### **Пример расчета статистики с накоплением между батчами**\n",
    "Обрабатывается batch, размером 10 строк. Рассчитывается среднее арифметическое значение.  \n",
    " - для 1-го батча - расчет только для него (и запоминает значение)\n",
    " - для 2-го батча - расчет для 1-го и 2-го совокупно\n",
    " - для 3-го батча - расчет для 1-го, 2-го ... по N-ый совокупно\n",
    "\n",
    "⚠️ **Функция обменивается данными только внутри батчей одной партиции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8241cc10-d4d3-4166-a47e-dc34956fee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройки\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdfb03a5-a171-4472-8c8a-560a539d57de",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def partitiom_statistics(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Вычисляет статистики с накоплением между батчами\"\"\"\n",
    "    running_sum = 0\n",
    "    running_count = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        running_sum += batch.sum()\n",
    "        running_count += len(batch)\n",
    "        print(f\"Обрабатывается батч размером: {len(batch)}\")                    \n",
    "        print(f\"running_sum = {running_sum}, running_count = {running_count}\")\n",
    "        # Возвращаем среднее значение на текущий момент\n",
    "        current_mean = running_sum / running_count\n",
    "        yield pd.Series([current_mean] * len(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3b97cf8-31ab-4081-a978-5d31c3b618b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер батча Arrow: 10\n",
      "Количество partitions: 5\n",
      "Размеры partitions: [801, 800, 799, 799, 801]\n"
     ]
    }
   ],
   "source": [
    "print(\"Размер батча Arrow:\", spark.conf.get(\"spark.sql.execution.arrow.maxRecordsPerBatch\"))\n",
    "print(\"Количество partitions:\", dfSample.rdd.getNumPartitions())\n",
    "print(\"Размеры partitions:\", dfSample.rdd.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a02b172-0d30-4004-b545-08c69e4b8221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------+\n",
      "|rowid| value|running_mean|\n",
      "+-----+------+------------+\n",
      "| 1728|1728.0|       918.7|\n",
      "|  174| 174.0|       918.7|\n",
      "|  138| 138.0|       918.7|\n",
      "| 1758|1758.0|       918.7|\n",
      "| 1719|1719.0|       918.7|\n",
      "| 1310|1310.0|       918.7|\n",
      "|  202| 202.0|       918.7|\n",
      "| 1375|1375.0|       918.7|\n",
      "|  395| 395.0|       918.7|\n",
      "|  388| 388.0|       918.7|\n",
      "+-----+------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обрабатывается батч размером: 10\n",
      "running_sum = 9187.0, running_count = 10\n",
      "Обрабатывается батч размером: 1\n",
      "running_sum = 10927.0, running_count = 11\n"
     ]
    }
   ],
   "source": [
    "# Применение для 11 строк - вывод для одного батча\n",
    "result = dfSample.select(\"rowid\", col(\"value\"), partitiom_statistics(col(\"value\")).alias(\"running_mean\"))\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee6f6bdb-0a20-421a-b55e-cb617e284316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------+\n",
      "|rowid| value|running_mean|\n",
      "+-----+------+------------+\n",
      "| 1728|1728.0|       918.7|\n",
      "|  174| 174.0|       918.7|\n",
      "|  138| 138.0|       918.7|\n",
      "| 1758|1758.0|       918.7|\n",
      "| 1719|1719.0|       918.7|\n",
      "| 1310|1310.0|       918.7|\n",
      "|  202| 202.0|       918.7|\n",
      "| 1375|1375.0|       918.7|\n",
      "|  395| 395.0|       918.7|\n",
      "|  388| 388.0|       918.7|\n",
      "| 1740|1740.0|      917.65|\n",
      "| 1369|1369.0|      917.65|\n",
      "|  582| 582.0|      917.65|\n",
      "|  298| 298.0|      917.65|\n",
      "|  920| 920.0|      917.65|\n",
      "|  641| 641.0|      917.65|\n",
      "| 1054|1054.0|      917.65|\n",
      "|  291| 291.0|      917.65|\n",
      "|  406| 406.0|      917.65|\n",
      "| 1865|1865.0|      917.65|\n",
      "+-----+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обрабатывается батч размером: 10\n",
      "running_sum = 9187.0, running_count = 10\n",
      "Обрабатывается батч размером: 10\n",
      "running_sum = 18353.0, running_count = 20\n",
      "Обрабатывается батч размером: 1\n",
      "running_sum = 19347.0, running_count = 21\n"
     ]
    }
   ],
   "source": [
    "# Применение для 21 строки - вывод для двух батчей\n",
    "result = dfSample.select(\"rowid\", col(\"value\"), partitiom_statistics(col(\"value\")).alias(\"running_mean\"))\n",
    "result.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894498b1-325b-4a4d-9755-dacf5c8fd23b",
   "metadata": {},
   "source": [
    "#### **Пример расчета накопительной суммы (Cumulative Sum)** и **Скользящего среднего** (между батчами)\n",
    "Еще один пример расчета с учетом состояний между батчами  \n",
    "\n",
    "⚠️ **Функция обменивается данными только внутри батчей одной партиции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "413efac2-b1ea-4968-8b05-5ed1fb2d8f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создается тестовый DataFrame 5 партиций, размер батча: 10 \n",
    "data = [(i, float(i)) for i in range(1, 31)]\n",
    "df_example_2 = spark.createDataFrame(data, [\"rowid\", \"value\"]).repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b382684-e26c-4b20-9349-2d93a71b59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def cross_batch_cumulative_sum(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Накопительную сумму через батчи\"\"\"\n",
    "    running_total = 0.0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        batch_cumsum = batch.cumsum()            # Вычисляется cumsum внутри батча\n",
    "        result = batch_cumsum + running_total    # Накопленное значение с предыдущих батчей\n",
    "        # Обновление состояни/ для следующего батча\n",
    "        running_total += batch.sum()\n",
    "        yield result\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def cross_batch_moving_avg(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Скользящее среднее с окном, пересекающим батчи\"\"\"\n",
    "    window_size = 5\n",
    "    window_buffer = deque(maxlen=window_size)\n",
    "    \n",
    "    for batch in iterator:\n",
    "        results = []\n",
    "        \n",
    "        for value in batch:\n",
    "            window_buffer.append(value)\n",
    "            current_avg = sum(window_buffer) / len(window_buffer) # Вычисляется среднее по текущему окну\n",
    "            results.append(current_avg)\n",
    "        \n",
    "        yield pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "746837e4-569e-4acb-87f0-c32abe3e5e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер батча Arrow: 10\n",
      "Количество partitions: 5\n",
      "Размеры partitions: [6, 6, 6, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"Размер батча Arrow:\", spark.conf.get(\"spark.sql.execution.arrow.maxRecordsPerBatch\"))\n",
    "print(\"Количество partitions:\", df_example_2.rdd.getNumPartitions())\n",
    "print(\"Размеры partitions:\", df_example_2.rdd.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ff6d3-5baa-4cfd-ab37-640a8a911c84",
   "metadata": {},
   "source": [
    "**Применение на 10 батчах в каждой из 5 партиций**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd6327ba-f0cd-486a-bec9-2f826957bab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------+-----------------+\n",
      "|rowid|value|cumulative_sum|       moving_avg|\n",
      "+-----+-----+--------------+-----------------+\n",
      "|   11| 11.0|          11.0|             11.0|\n",
      "|    9|  9.0|          20.0|             10.0|\n",
      "|    8|  8.0|          28.0|9.333333333333334|\n",
      "|   28| 28.0|          56.0|             14.0|\n",
      "|   19| 19.0|          75.0|             15.0|\n",
      "|   25| 25.0|         100.0|             17.8|\n",
      "|    6|  6.0|           6.0|              6.0|\n",
      "|   15| 15.0|          21.0|             10.5|\n",
      "|   12| 12.0|          33.0|             11.0|\n",
      "|   30| 30.0|          63.0|            15.75|\n",
      "|   16| 16.0|          79.0|             15.8|\n",
      "|   29| 29.0|         108.0|             20.4|\n",
      "|    3|  3.0|           3.0|              3.0|\n",
      "|    4|  4.0|           7.0|              3.5|\n",
      "|    7|  7.0|          14.0|4.666666666666667|\n",
      "|   22| 22.0|          36.0|              9.0|\n",
      "|   27| 27.0|          63.0|             12.6|\n",
      "|   24| 24.0|          87.0|             16.8|\n",
      "|    1|  1.0|           1.0|              1.0|\n",
      "|    2|  2.0|           3.0|              1.5|\n",
      "+-----+-----+--------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Применение r\n",
    "df_example_2.select(\"rowid\", \"value\"\n",
    "                    ,cross_batch_cumulative_sum(col(\"value\")).alias(\"cumulative_sum\")\n",
    "                    ,cross_batch_moving_avg(col(\"value\")).alias(\"moving_avg\")\n",
    "                    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3e93c-b08b-403a-9df8-c8ea02dab43c",
   "metadata": {},
   "source": [
    "**Применение на 10 батчах для одной партиции** (сортировка приводит DataFrame к одной партиции)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ea94238-5804-4c67-9777-4a8e00a6f822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер батча Arrow: 10\n",
      "Количество partitions: 1\n",
      "Размеры partitions: [30]\n"
     ]
    }
   ],
   "source": [
    "print(\"Размер батча Arrow:\", spark.conf.get(\"spark.sql.execution.arrow.maxRecordsPerBatch\"))\n",
    "print(\"Количество partitions:\", df_example_2.orderBy(\"rowid\").rdd.getNumPartitions())\n",
    "print(\"Размеры partitions:\", df_example_2.orderBy(\"rowid\").rdd.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "569995a6-f225-4c62-98e0-fcb6cc1deab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------+----------+\n",
      "|rowid|value|cumulative_sum|moving_avg|\n",
      "+-----+-----+--------------+----------+\n",
      "|    1|  1.0|           1.0|       1.0|\n",
      "|    2|  2.0|           3.0|       1.5|\n",
      "|    3|  3.0|           6.0|       2.0|\n",
      "|    4|  4.0|          10.0|       2.5|\n",
      "|    5|  5.0|          15.0|       3.0|\n",
      "|    6|  6.0|          21.0|       4.0|\n",
      "|    7|  7.0|          28.0|       5.0|\n",
      "|    8|  8.0|          36.0|       6.0|\n",
      "|    9|  9.0|          45.0|       7.0|\n",
      "|   10| 10.0|          55.0|       8.0|\n",
      "|   11| 11.0|          66.0|       9.0|\n",
      "|   12| 12.0|          78.0|      10.0|\n",
      "|   13| 13.0|          91.0|      11.0|\n",
      "|   14| 14.0|         105.0|      12.0|\n",
      "|   15| 15.0|         120.0|      13.0|\n",
      "|   16| 16.0|         136.0|      14.0|\n",
      "|   17| 17.0|         153.0|      15.0|\n",
      "|   18| 18.0|         171.0|      16.0|\n",
      "|   19| 19.0|         190.0|      17.0|\n",
      "|   20| 20.0|         210.0|      18.0|\n",
      "+-----+-----+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Применение c сортировкой\n",
    "df_example_2.orderBy(\"rowid\")\\\n",
    "            .select(\"rowid\", \"value\"\n",
    "                    ,cross_batch_cumulative_sum(col(\"value\")).alias(\"cumulative_sum\")\n",
    "                    ,cross_batch_moving_avg(col(\"value\")).alias(\"moving_avg\")\n",
    "                   ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb6d33-4a81-4c68-8818-f433c5dfc6d3",
   "metadata": {},
   "source": [
    "#### **Пример реализацмм счетчика уникальных значений**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56628bb8-d45a-47db-a00c-b072db6d95e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные с повторениями\n",
    "data = [(1, \"A\"), (2, \"B\"), (3, \"A\"), (4, \"C\"), (5, \"B\"), (6, \"D\"), (7, \"E\"), (8, \"F\")]\n",
    "df_unique_count_example = spark.createDataFrame(data, [\"rowid\", \"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "766b3799-4521-4467-a132-85aec2a05c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер батча Arrow: 10\n",
      "Количество partitions: 2\n",
      "Размеры partitions: [4, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"Размер батча Arrow:\", spark.conf.get(\"spark.sql.execution.arrow.maxRecordsPerBatch\"))\n",
    "print(\"Количество partitions:\", df_unique_count_example.rdd.getNumPartitions())\n",
    "print(\"Размеры partitions:\", df_unique_count_example.rdd.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e8e245f-17a5-4b56-9071-004d853fddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"long\")\n",
    "def unique_count_stateful(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Подсчет уникальных значений с состоянием\"\"\"\n",
    "    seen_values = set()  # Состояние: множество уже провереных значений\n",
    "    \n",
    "    for batch in iterator:\n",
    "        results = []\n",
    "        \n",
    "        for value in batch:\n",
    "            seen_values.add(value)\n",
    "            # Возвращаем текущее количество уникальных значений\n",
    "            results.append(len(seen_values))\n",
    "        \n",
    "        yield pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4843c99-4b66-4077-a4ef-e17a436c7ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+\n",
      "|rowid|value|unique_count|\n",
      "+-----+-----+------------+\n",
      "|    1|    A|           1|\n",
      "|    2|    B|           2|\n",
      "|    3|    A|           2|\n",
      "|    4|    C|           3|\n",
      "|    5|    B|           1|\n",
      "|    6|    D|           2|\n",
      "|    7|    E|           3|\n",
      "|    8|    F|           4|\n",
      "+-----+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = df_unique_count_example.select(\"rowid\", \"value\", \n",
    "                                         unique_count_stateful(col(\"value\")).alias(\"unique_count\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2603fa-d162-4623-a985-a88c5f2ad9a8",
   "metadata": {},
   "source": [
    "#### **Пример реализацмм обнаружения аномалий с адаптивным порогом**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d19d11bb-6dfc-4b80-a487-323dfda09263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSet с аномалиями\n",
    "np.random.seed(42)\n",
    "normal_data = list(np.random.normal(50, 10, 18))\n",
    "anomaly_data = [150, 200]  # Аномалии\n",
    "all_data = normal_data + anomaly_data\n",
    "\n",
    "data = [(i+1, float(val)) for i, val in enumerate(all_data)]\n",
    "dfAnomalyDate = spark.createDataFrame(data, [\"rowid\", \"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4164e7e3-7505-4457-83e6-071a836847d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер батча Arrow: 10\n",
      "Количество partitions: 2\n",
      "Размеры partitions: [10, 10]\n"
     ]
    }
   ],
   "source": [
    "print(\"Размер батча Arrow:\", spark.conf.get(\"spark.sql.execution.arrow.maxRecordsPerBatch\"))\n",
    "print(\"Количество partitions:\", dfAnomalyDate.rdd.getNumPartitions())\n",
    "print(\"Размеры partitions:\", dfAnomalyDate.rdd.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "859a5e40-ed43-493b-a574-279696ef65e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "@pandas_udf(\"boolean\")\n",
    "def adaptive_anomaly_detection(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Обнаружение аномалий с адаптивным порогом\"\"\"\n",
    "    # Состояние: статистики для адаптации порога\n",
    "    running_mean = 0.0\n",
    "    running_var = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        results = []\n",
    "        \n",
    "        for value in batch:\n",
    "            count += 1\n",
    "            \n",
    "            # Обновляем running statistics (Welford's algorithm)\n",
    "            delta = value - running_mean\n",
    "            running_mean += delta / count\n",
    "            delta2 = value - running_mean\n",
    "            running_var += delta * delta2\n",
    "            \n",
    "            # Вычисляем текущее стандартное отклонение\n",
    "            if count > 1:\n",
    "                current_std = np.sqrt(running_var / (count - 1))\n",
    "                threshold = 2 * current_std                \n",
    "                # Проверяем на аномалию\n",
    "                is_anomaly = abs(value - running_mean) > threshold\n",
    "            else:\n",
    "                is_anomaly = False\n",
    "            \n",
    "            results.append(is_anomaly)\n",
    "        \n",
    "        yield pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2aaa2e12-46c3-4ac6-b71d-afad4176f01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+----------+\n",
      "|rowid|             value|is_anomaly|\n",
      "+-----+------------------+----------+\n",
      "|    1| 54.96714153011233|     false|\n",
      "|    2| 48.61735698828815|     false|\n",
      "|    3| 56.47688538100692|     false|\n",
      "|    4| 65.23029856408026|     false|\n",
      "|    5|47.658466252766644|     false|\n",
      "|    6|  47.6586304305082|     false|\n",
      "|    7| 65.79212815507391|     false|\n",
      "|    8| 57.67434729152909|     false|\n",
      "|    9| 45.30525614065048|     false|\n",
      "|   10| 55.42560043585965|     false|\n",
      "|   11|45.365823071875376|     false|\n",
      "|   12| 45.34270246429743|     false|\n",
      "|   13| 52.41962271566034|     false|\n",
      "|   14| 30.86719755342202|     false|\n",
      "|   15| 32.75082167486967|     false|\n",
      "|   16| 44.37712470759027|     false|\n",
      "|   17| 39.87168879665576|     false|\n",
      "|   18|53.142473325952736|     false|\n",
      "|   19|             150.0|      true|\n",
      "|   20|             200.0|      true|\n",
      "+-----+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = dfAnomalyDate.select(\"rowid\", \"value\", \n",
    "                  adaptive_anomaly_detection(col(\"value\")).alias(\"is_anomaly\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2e6f1-9d72-4687-bf11-0bf63a653834",
   "metadata": {},
   "source": [
    "### 3. **Обработка с кэшированием**\n",
    "Мощный механизм оптимизации обработки данных с помошью **pandas_udf** (SCALAR_ITER)  \n",
    "**Основной принцип:**\n",
    "Сохранение расчитанных результатов в **cache** (ключ-знчение) , для избегания повтрных расчетов\n",
    "```python\n",
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def processing_with_cache(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Кэш для избежания повторных вычислений\n",
    "    cache = {}\n",
    "    \n",
    "    def processing_code(value):\n",
    "        if value in cache:\n",
    "            return cache[value]\n",
    "        \n",
    "        if value is None:\n",
    "            result = None\n",
    "        else:\n",
    "            result =  <Сложная обработка>\n",
    "        \n",
    "        cache[value] = result\n",
    "        return result\n",
    "    \n",
    "    for batch in iterator:\n",
    "        processed_batch = batch.apply(processing_code)\n",
    "        yield processed_batch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d604a938-1d7f-4d99-8471-135e7ba5f75c",
   "metadata": {},
   "source": [
    "#### **Простой пример с кэшироанием**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1cea51e-72eb-4865-a95a-691bab60a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные с повторениями для демонстрации кэширования\n",
    "text_data = [\n",
    "    (1, \"APACHE\"), (2, \"HIVE\"), (3, \"APACHE\"),  # APACHE повторяется\n",
    "    (4, \"PYTHON\"), (5, \"HIVE\"), (6, \"SPARK\")    # HIVE повторяется\n",
    "]\n",
    "df_text = spark.createDataFrame(text_data, [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a758deef-4a2b-4304-9cd7-91fa3a2d30cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройки\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"3\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "801a9f97-1865-493b-a4db-a9f48f0b09e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер батча Arrow: 3\n",
      "Количество partitions: 2\n",
      "Размеры partitions: [3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Размер батча Arrow:\", spark.conf.get(\"spark.sql.execution.arrow.maxRecordsPerBatch\"))\n",
    "print(\"Количество partitions:\", df_text.rdd.getNumPartitions())\n",
    "print(\"Размеры partitions:\", df_text.rdd.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bba8e15d-9f9f-4191-aa1e-cfadf7c3f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"string\")\n",
    "def incremental_text_processing(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Инкрементальная обработка текста с кэшем\"\"\"\n",
    "    # Состояние: кэш обработанных значений\n",
    "    processing_cache = {}\n",
    "    processing_count = 0\n",
    "    \n",
    "    def expensive_text_operation(text):\n",
    "        \"\"\"Имитация дорогостоящей операции обработки текста\"\"\"\n",
    "        return text.upper().replace('A', '@').replace('E', '3')\n",
    "    \n",
    "    for batch in iterator:\n",
    "        results = []\n",
    "        \n",
    "        for text in batch:\n",
    "            if text in processing_cache:\n",
    "                # Кэшированный результат\n",
    "                result = processing_cache[text]\n",
    "            else:\n",
    "                # Выполнение дорогой операции\n",
    "                result = expensive_text_operation(text)\n",
    "                processing_cache[text] = result\n",
    "                processing_count += 1\n",
    "        \n",
    "            results.append(result)\n",
    "        \n",
    "        yield pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf2e1420-b652-4935-8c7b-13316ca46287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+\n",
      "| id|  text|processed|\n",
      "+---+------+---------+\n",
      "|  1|APACHE|   @P@CH3|\n",
      "|  2|  HIVE|     HIV3|\n",
      "|  3|APACHE|   @P@CH3|\n",
      "|  4|PYTHON|   PYTHON|\n",
      "|  5|  HIVE|     HIV3|\n",
      "|  6| SPARK|    SP@RK|\n",
      "+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = df_text.select(\"id\", \"text\", \n",
    "                       incremental_text_processing(col(\"text\")).alias(\"processed\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff644b0f-d17d-4a0f-860e-08dd65b9f652",
   "metadata": {},
   "source": [
    "#### **Пример с кэшированием для операции с регулярными выражениями**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c351969b-953d-4502-b00c-67ba68483a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def text_processing_with_cache(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Обработка текста с кэшированием результатов\"\"\"\n",
    "    \n",
    "    # Кэш для избежания повторных вычислений\n",
    "    cache = {}\n",
    "    \n",
    "    def clean_text(text):\n",
    "        if text in cache:\n",
    "            return cache[text]\n",
    "        \n",
    "        if text is None:\n",
    "            result = None\n",
    "        else:\n",
    "            # Сложная обработка текста r'[^a-zA-Z0-9\\s]'\n",
    "            result = re.sub(r'[^a-zA-Z0-9а-яА-ЯёЁ\\s]', '', str(text)).strip().upper()\n",
    "\n",
    "        cache[text] = result\n",
    "        return result\n",
    "    \n",
    "    for batch in iterator:\n",
    "        processed_batch = batch.apply(clean_text)\n",
    "        yield processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c97ef8a3-bf01-43cc-bc15-804064cab29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------+-------------------------------+\n",
      "|rowid|str_value                            |cleaned                        |\n",
      "+-----+-------------------------------------+-------------------------------+\n",
      "|1    |Hello, World!!!                      |HELLO WORLD                    |\n",
      "|2    |Python@#$%^&*()                      |PYTHON                         |\n",
      "|3    |Привет, мир!!! 123                   |ПРИВЕТ МИР 123                 |\n",
      "|4    |NULL                                 |NULL                           |\n",
      "|5    |код (130) Номер 244=-55-56\")         |КОД 130 НОМЕР 2445556          |\n",
      "|6    |## Ключевые особенности 'SCALAR UDF:'|КЛЮЧЕВЫЕ ОСОБЕННОСТИ SCALAR UDF|\n",
      "|7    |**Векто(р)и'\\зованные опе%%%рации**  |ВЕКТОРИЗОВАННЫЕ ОПЕРАЦИИ       |\n",
      "|8    |?*;'';&()_                           |                               |\n",
      "|9    |Simple String №1                     |SIMPLE STRING 1                |\n",
      "|10   |Simple String №2                     |SIMPLE STRING 2                |\n",
      "+-----+-------------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = dfData.select(\"rowid\", \"str_value\", text_processing_with_cache(col(\"str_value\")).alias(\"cleaned\"))\n",
    "result.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3060fd4a-45cb-48b6-a674-7ac2d20b54d1",
   "metadata": {},
   "source": [
    "#### **Сравнение обработки строк с кэшированием и без**  \n",
    "Допустим есть какая-то операция с текстом, длительностью 0.1 сек, и текстовое поле с повторяющимися значениями "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "426db510-2d39-4fdd-8e62-131da89c5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALAR_ITER с кэшированием (эффективно)\n",
    "@pandas_udf(\"string\")\n",
    "def cached_text_processing(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Обработчик текста с кэшированием\"\"\"\n",
    "    cache = {}  # Кэш живет между всеми батчами\n",
    "    \n",
    "    def process_text(text):\n",
    "        if text in cache:\n",
    "            return cache[text]\n",
    "            \n",
    "        # Дорогая обработка\n",
    "        time.sleep(.1)\n",
    "        result = text.upper() if text else None\n",
    "        cache[text] = result\n",
    "        return result\n",
    "    \n",
    "    for batch in iterator:\n",
    "        processed = batch.apply(process_text)\n",
    "        yield processed\n",
    "\n",
    "# SCALAR без кэширования (неэффективно для повторяющихся данных)\n",
    "@pandas_udf(\"string\")\n",
    "def uncached_text_processing(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Обработчик текста без кэширования\"\"\"   \n",
    "    # Кэша нет\n",
    "    def process_text(text):\n",
    "        time.sleep(.1)\n",
    "        return text.upper() if text else None\n",
    "    \n",
    "    return series.apply(process_text)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc636c-768e-483b-b310-6c2bf7d8faef",
   "metadata": {},
   "source": [
    "**DataSet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26f7fbd0-25b8-4f0f-aeda-2810daeaebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 500\n",
      "+-------+-----+\n",
      "|   text|count|\n",
      "+-------+-----+\n",
      "|jupyter|   82|\n",
      "| apache|   89|\n",
      "|  spark|   85|\n",
      "|    hue|   86|\n",
      "|  oozie|   68|\n",
      "|   hive|   90|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_data(words=[], n=5000):\n",
    "    \"\"\"Генератор n-значений из списка words\"\"\"   \n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        text = random.choice(words)\n",
    "        data.append((i + 1, text))\n",
    "    return data\n",
    "    \n",
    "# Список категорий\n",
    "words = [\"apache\", \"spark\", \"hive\", \"oozie\", \"jupyter\", \"hue\"]\n",
    "# Генерация данных - n строк\n",
    "data = generate_data(words, 500)\n",
    "\n",
    "# Определение схемы\n",
    "schema = StructType([\n",
    "    StructField(\"rowid\", IntegerType(), True),\n",
    "    StructField(\"text\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Создание DataFrame\n",
    "dfCacheTest = spark.createDataFrame(data, schema).repartition(2)\n",
    "print(f'count: {dfCacheTest.count()}')\n",
    "dfCacheTest.groupBy(\"text\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce2b1e0d-ba3a-465b-8e3d-0bca71d90153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройки\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"50\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae353022-adb3-435c-bf4a-4cdfc8aebdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер батча Arrow: 50\n",
      "Количество partitions: 2\n",
      "Размеры partitions: [250, 250]\n"
     ]
    }
   ],
   "source": [
    "print(\"Размер батча Arrow:\", spark.conf.get(\"spark.sql.execution.arrow.maxRecordsPerBatch\"))\n",
    "print(\"Количество partitions:\", dfCacheTest.rdd.getNumPartitions())\n",
    "print(\"Размеры partitions:\", dfCacheTest.rdd.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b6e7c33e-0f70-43a7-8667-a5edca741815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 ms, sys: 1.98 ms, total: 13.3 ms\n",
      "Wall time: 25.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = dfCacheTest.select(\"rowid\", uncached_text_processing(col(\"text\")).alias(\"text\"))\n",
    "ret = result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e658f564-32d7-448a-939e-713d11c2dfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 ms, sys: 1.87 ms, total: 13.1 ms\n",
      "Wall time: 713 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = dfCacheTest.select(\"rowid\", cached_text_processing(col(\"text\")).alias(\"text\"))\n",
    "ret = result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ec802-3107-47bf-9637-6efc6d86325d",
   "metadata": {},
   "source": [
    "### 3. Обработка с внешними ресурсами\n",
    "Некоторый ресурс загружается/рассчитывается один раз для каждой паритиции и пременяется к батчам расчета\n",
    " - создается тестовая модель и сохраняется в фвйл\n",
    " - загружается модель и применяется (к батчам)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6f2700e-3523-4bec-b4b5-a6a191f837c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9ef8e-5316-4cc5-aaaf-c8e288701c74",
   "metadata": {},
   "source": [
    "#### **Пример батчевого применения модели с передачей 3х фичей-колонок через параметр**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ca1ae29e-c340-48e4-af44-c7f960aebed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель сохранена в /tmp/model_weights.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cоздаем и сохраняем модель\n",
    "def create_and_save_model():\n",
    "    \"\"\"Создает и сохраняет модель в файл\"\"\"\n",
    "\n",
    "    model_weights = {\n",
    "            'feature1_weight': 2.4,\n",
    "            'feature2_weight': 1.2,\n",
    "            'feature3_weight': 0.7,\n",
    "            'bias': 0.4\n",
    "        }\n",
    "    # Сохраняем в pickle файл\n",
    "    with open('/tmp/model_weights.pkl', 'wb') as f:\n",
    "        pickle.dump(model_weights, f)\n",
    "    \n",
    "    print(\"Модель сохранена в /tmp/model_weights.pkl\")\n",
    "\n",
    "# Создаем модель\n",
    "create_and_save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fcb1f8e4-92a1-43ef-8a6d-f20b27c0799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def ml_prediction_batched(iterator: Iterator[tuple[pd.Series, pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Применяет ML модель к батчам данных с 3 фичами\"\"\"\n",
    "    \n",
    "    # Инициализация модели один раз для всех батчей\n",
    "    model_path = '/tmp/model_weights.pkl'\n",
    "    if os.path.exists(model_path):\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_weights = pickle.load(f)\n",
    "        print(f\"Модель загружена из {model_path}: {model_weights}\")\n",
    "    else:\n",
    "        # Fallback модель: веса для 3 фичей + bias\n",
    "        model_weights = {\n",
    "            'feature1_weight': 0.0,\n",
    "            'feature2_weight': 0.0,\n",
    "            'feature3_weight': 0.0,\n",
    "            'bias': 0.0\n",
    "        }\n",
    "        print(\"Файл модели не найден, используется fallback модель\")\n",
    "    \n",
    "    for feature1_batch, feature2_batch, feature3_batch in iterator:\n",
    "        # Применяем линейную модель: w1*f1 + w2*f2 + w3*f3 + bias\n",
    "        predictions = (\n",
    "            feature1_batch * model_weights['feature1_weight'] + \n",
    "            feature2_batch * model_weights['feature2_weight'] + \n",
    "            feature3_batch * model_weights['feature3_weight'] + \n",
    "            model_weights['bias']\n",
    "        )\n",
    "        yield predictions.round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2389b7a3-34ca-455f-aede-662aa7896f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем тестовые данные\n",
    "test_df = spark.range(100).toDF(\"id\") \\\n",
    "    .withColumn(\"feature1\", rand() * 10) \\\n",
    "    .withColumn(\"feature2\", rand() * 5) \\\n",
    "    .withColumn(\"feature3\", rand() * 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65e2be2a-7e10-4030-bb99-89f72f0648e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'),\n",
       " ('feature1', 'double'),\n",
       " ('feature2', 'double'),\n",
       " ('feature3', 'double')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e07813c-b05d-4ec8-ab03-38538126e3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+------------------+----------+\n",
      "| id|          feature1|          feature2|          feature3|prediction|\n",
      "+---+------------------+------------------+------------------+----------+\n",
      "|  0|3.9232266873712893|1.3831624950487542|0.8473818464765851|  12.06871|\n",
      "|  1| 1.879715341767808| 4.793458188392268|1.4133132776998298|  11.65279|\n",
      "|  2|  9.96196576952044|3.7851142788807723|0.7644882033571369|    29.386|\n",
      "|  3| 7.661686053597718| 3.038556668302106|0.9796526608645141|  23.12007|\n",
      "|  4| 1.101099703869317|2.7244348337705286|  2.07428815201402|   7.76396|\n",
      "|  5|1.0976073284409449| 4.521899926604037|1.8267422973815974|   9.73926|\n",
      "|  6| 7.564336171391773|2.7890481227500272|1.4162589368737033|  22.89265|\n",
      "|  7| 8.683496072226472|1.8356516256531052| 1.767602527649105|  24.68049|\n",
      "|  8|1.0376579659324392| 2.729749257998471|0.8856184230999437|   6.78601|\n",
      "|  9| 6.249566348215032|1.7044574239431172|1.1813544432570073|  18.27126|\n",
      "+---+------------------+------------------+------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Модель загружена из /tmp/model_weights.pkl: {'feature1_weight': 2.4, 'feature2_weight': 1.2, 'feature3_weight': 0.7, 'bias': 0.4}\n"
     ]
    }
   ],
   "source": [
    "# Применяем UDF\n",
    "result_df = test_df.withColumn(\n",
    "    \"prediction\", \n",
    "    ml_prediction_batched(col(\"feature1\"), col(\"feature2\"), col(\"feature3\"))\n",
    ")\n",
    "result_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2308ad-4841-4eec-9bfb-fe8962a3997c",
   "metadata": {},
   "source": [
    "#### **Пример батчевого применения модели с использованием DataFrame внутри UDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8131e347-f02b-42c9-9a81-3718f0403f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель сохранена в /tmp/model_weights.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cоздаем и сохраняем модель\n",
    "def create_and_save_model():\n",
    "    \"\"\"Создает и сохраняет модель в файл\"\"\"\n",
    "\n",
    "    model_weights = {'weights': [2.4, 1.2, 0.7, 0.4], 'bias': 0.4} \n",
    "    # Сохраняем в pickle файл\n",
    "    with open('/tmp/model_weights.pkl', 'wb') as f:\n",
    "        pickle.dump(model_weights, f)\n",
    "    \n",
    "    print(\"Модель сохранена в /tmp/model_weights.pkl\")\n",
    "\n",
    "# Создаем модель\n",
    "create_and_save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4df4d9a7-0727-4a0c-821d-e3722c8c797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def ml_prediction_batched(iterator: Iterator[tuple[pd.Series, pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Применяет ML модель к батчам данных используя pandas DataFrame\"\"\"\n",
    "    \n",
    "    # Инициализация модели\n",
    "    model_path = '/tmp/model_weights.pkl'\n",
    "    if os.path.exists(model_path):\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_config = pickle.load(f)\n",
    "        weights = model_config['weights']\n",
    "        bias = model_config['bias']\n",
    "        print(f\"Модель загружена: weights={weights}, bias={bias}\")\n",
    "    else:\n",
    "        weights = [0.6, 0.3, 0.1]\n",
    "        bias = 0.5\n",
    "        print(\"Используется fallback модель\")\n",
    "    \n",
    "    for feature1_batch, feature2_batch, feature3_batch in iterator:\n",
    "        # Создаем DataFrame из батчей\n",
    "        batch_df = pd.DataFrame({\n",
    "            'f1': feature1_batch,\n",
    "            'f2': feature2_batch, \n",
    "            'f3': feature3_batch\n",
    "        })\n",
    "        \n",
    "        # Применяем модель\n",
    "        predictions = (\n",
    "            batch_df['f1'] * weights[0] + \n",
    "            batch_df['f2'] * weights[1] + \n",
    "            batch_df['f3'] * weights[2] + \n",
    "            bias\n",
    "        )\n",
    "        \n",
    "        yield predictions.round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "32222aea-1de7-4fd6-9333-dde3fc73c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+------------------+----------+\n",
      "| id|          feature1|          feature2|          feature3|prediction|\n",
      "+---+------------------+------------------+------------------+----------+\n",
      "|  0|3.9232266873712893|1.3831624950487542|0.8473818464765851|  12.06871|\n",
      "|  1| 1.879715341767808| 4.793458188392268|1.4133132776998298|  11.65279|\n",
      "|  2|  9.96196576952044|3.7851142788807723|0.7644882033571369|    29.386|\n",
      "|  3| 7.661686053597718| 3.038556668302106|0.9796526608645141|  23.12007|\n",
      "|  4| 1.101099703869317|2.7244348337705286|  2.07428815201402|   7.76396|\n",
      "|  5|1.0976073284409449| 4.521899926604037|1.8267422973815974|   9.73926|\n",
      "|  6| 7.564336171391773|2.7890481227500272|1.4162589368737033|  22.89265|\n",
      "|  7| 8.683496072226472|1.8356516256531052| 1.767602527649105|  24.68049|\n",
      "|  8|1.0376579659324392| 2.729749257998471|0.8856184230999437|   6.78601|\n",
      "|  9| 6.249566348215032|1.7044574239431172|1.1813544432570073|  18.27126|\n",
      "+---+------------------+------------------+------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Модель загружена: weights=[2.4, 1.2, 0.7, 0.4], bias=0.4\n"
     ]
    }
   ],
   "source": [
    "# Применяем UDF\n",
    "result_df = test_df.withColumn(\n",
    "    \"prediction\", \n",
    "    ml_prediction_batched(col(\"feature1\"), col(\"feature2\"), col(\"feature3\"))\n",
    ")\n",
    "result_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d58c3-bd9c-43e0-a2c0-1d6256caad6e",
   "metadata": {},
   "source": [
    "#### **Пример применения иодели с обработкой ошибок**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b0833912-9edc-47ff-9f2b-3fc82984d66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель сохранена в /tmp/model_weights.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cоздаем и сохраняем модель\n",
    "def create_and_save_model():\n",
    "    \"\"\"Создает и сохраняет модель в файл\"\"\"\n",
    "\n",
    "    model_weights = [2.4, 1.2, 0.7, 0.4] \n",
    "    # Сохраняем в pickle файл\n",
    "    with open('/tmp/model_weights.pkl', 'wb') as f:\n",
    "        pickle.dump(model_weights, f)\n",
    "    \n",
    "    print(\"Модель сохранена в /tmp/model_weights.pkl\")\n",
    "\n",
    "# Создаем модель\n",
    "create_and_save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d6bcf0a-ad6c-4b3d-84bc-a811ecbadb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def ml_prediction_batched_safe(iterator: Iterator[tuple[pd.Series, pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Версия с обработкой ошибок\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Загрузка модели\n",
    "        model_path = '/tmp/model_weights.pkl'\n",
    "        if os.path.exists(model_path):\n",
    "            with open(model_path, 'rb') as f:\n",
    "                model_weights = pickle.load(f)\n",
    "        else:\n",
    "            model_weights = [0.33, 0.33, 0.34, 0.0]  # равные веса + нулевой bias\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка загрузки модели: {e}\")\n",
    "        model_weights = [1.0, 0.0, 0.0, 0.0]  # fallback к первой фиче\n",
    "    \n",
    "    for feature1_batch, feature2_batch, feature3_batch in iterator:\n",
    "        try:\n",
    "            # Проверка на NaNи замена на 0\n",
    "            f1 = feature1_batch.fillna(0.0)\n",
    "            f2 = feature2_batch.fillna(0.0)\n",
    "            f3 = feature3_batch.fillna(0.0)\n",
    "            \n",
    "            # Применение модели\n",
    "            predictions = (\n",
    "                f1 * model_weights[0] + \n",
    "                f2 * model_weights[1] + \n",
    "                f3 * model_weights[2] + \n",
    "                model_weights[3]\n",
    "            )\n",
    "            \n",
    "            # Ограничиваем предсказания разумными пределами\n",
    "            predictions = predictions.clip(-100, 100)\n",
    "            \n",
    "            yield predictions.round(5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка в батче: {e}\")\n",
    "            # Возвращаем нули в случае ошибки\n",
    "            yield pd.Series([0.0] * len(feature1_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "462986e7-41aa-4ec1-8877-b8c6c9ebfc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+------------------+----------+\n",
      "| id|          feature1|          feature2|          feature3|prediction|\n",
      "+---+------------------+------------------+------------------+----------+\n",
      "|  0|3.9232266873712893|1.3831624950487542|0.8473818464765851|  12.06871|\n",
      "|  1| 1.879715341767808| 4.793458188392268|1.4133132776998298|  11.65279|\n",
      "|  2|  9.96196576952044|3.7851142788807723|0.7644882033571369|    29.386|\n",
      "|  3| 7.661686053597718| 3.038556668302106|0.9796526608645141|  23.12007|\n",
      "|  4| 1.101099703869317|2.7244348337705286|  2.07428815201402|   7.76396|\n",
      "|  5|1.0976073284409449| 4.521899926604037|1.8267422973815974|   9.73926|\n",
      "|  6| 7.564336171391773|2.7890481227500272|1.4162589368737033|  22.89265|\n",
      "|  7| 8.683496072226472|1.8356516256531052| 1.767602527649105|  24.68049|\n",
      "|  8|1.0376579659324392| 2.729749257998471|0.8856184230999437|   6.78601|\n",
      "|  9| 6.249566348215032|1.7044574239431172|1.1813544432570073|  18.27126|\n",
      "+---+------------------+------------------+------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Применяем UDF\n",
    "result_df = test_df.withColumn(\n",
    "    \"prediction\", \n",
    "    ml_prediction_batched_safe(col(\"feature1\"), col(\"feature2\"), col(\"feature3\"))\n",
    ")\n",
    "result_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45753e8-cbf8-4f11-ab50-2714a55c4a8f",
   "metadata": {},
   "source": [
    "### 4. Обработка с несколькими колонками через lambda\n",
    "Простой пример обработки нескольких параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e315396d-eaeb-461b-b157-f124e14e8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные с двумя колонками\n",
    "data = [(i, float(i), float(i * 0.1)) for i in range(1, 101)]\n",
    "dfTwoCols = spark.createDataFrame(data, [\"rowid\", \"value\", \"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "449b14bf-f219-411d-b8c1-d45e41e245cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def complex_calculation_iter(\n",
    "    iterator: Iterator[tuple[pd.Series, pd.Series]]\n",
    ") -> Iterator[pd.Series]:\n",
    "    \"\"\"Сложные вычисления с несколькими входными колонками\"\"\"\n",
    "    \n",
    "    for value_batch, weight_batch in iterator:\n",
    "        # Взвешенное преобразование\n",
    "        result = (value_batch * weight_batch).apply(lambda x: x ** 0.5 if x > 0 else 0)\n",
    "        yield result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "947cfcef-187a-4bbf-9929-11b638e223b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|rowid|    weighted_result|\n",
      "+-----+-------------------+\n",
      "|    1|0.31622776601683794|\n",
      "|    2| 0.6324555320336759|\n",
      "|    3| 0.9486832980505139|\n",
      "|    4| 1.2649110640673518|\n",
      "|    5| 1.5811388300841898|\n",
      "|    6| 1.8973665961010278|\n",
      "|    7| 2.2135943621178655|\n",
      "|    8| 2.5298221281347035|\n",
      "|    9| 2.8460498941515415|\n",
      "|   10| 3.1622776601683795|\n",
      "+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = dfTwoCols.select(\n",
    "    \"rowid\", \n",
    "    complex_calculation_iter(col(\"value\"), col(\"weight\")).alias(\"weighted_result\")\n",
    ")\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba40ae-bbab-4c37-91e2-357acf8bf310",
   "metadata": {},
   "source": [
    "### 5. Обработка временных рядов\n",
    "Сложный пример обработки со скользящим окном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0415b302-1c65-45e3-b425-8608cc400337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Временные ряды данных\n",
    "ts_data = [(i, float(10 + 5 * np.sin(i * 0.1) + np.random.randn())) \n",
    "           for i in range(1, 201)]\n",
    "df_time_series = spark.createDataFrame(ts_data, [\"rowid\", \"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "454f8c85-1929-46b5-b995-4237feb4c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def sliding_window_stats(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Вычисляет статистики скользящего окна между батчами\"\"\"\n",
    "    \n",
    "    window_buffer = []\n",
    "    window_size = 5\n",
    "    \n",
    "    for batch in iterator:\n",
    "        results = []\n",
    "        \n",
    "        for value in batch:\n",
    "            window_buffer.append(value)\n",
    "            \n",
    "            # Поддерживаем размер окна\n",
    "            if len(window_buffer) > window_size:\n",
    "                window_buffer.pop(0)\n",
    "            \n",
    "            # Вычисляем среднее по окну\n",
    "            window_mean = np.mean(window_buffer)\n",
    "            results.append(window_mean)\n",
    "        \n",
    "        yield pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fe53f9ee-fde6-40e8-a559-91cdf5294e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+\n",
      "|rowid|             value|          smoothed|\n",
      "+-----+------------------+------------------+\n",
      "|    1|  9.59114300771293|  9.59114300771293|\n",
      "|    2| 9.581042952640015| 9.586092980176472|\n",
      "|    3|12.943249802228252|10.705145254193733|\n",
      "|    4|11.721315411056718| 10.95918779340948|\n",
      "|    5| 12.46465589770894|11.260281414269372|\n",
      "|    6| 11.39846418076172| 11.62174564887913|\n",
      "|    7|12.676705711663274|12.240878200683781|\n",
      "|    8| 13.69770304420748|12.391768849079627|\n",
      "|    9|12.765640970715113|12.600633961011305|\n",
      "|   10|14.583052942385153| 13.02431336994655|\n",
      "|   11|13.855398110388373|13.515700155871878|\n",
      "|   12|14.368501680042856|13.854059349547793|\n",
      "|   13| 14.21608431485657|13.957735603677614|\n",
      "|   14| 16.77952683445124|14.760512776424838|\n",
      "|   15|14.973977708282339|14.838697729604274|\n",
      "+-----+------------------+------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = df_time_series.select(\"rowid\", \"value\", sliding_window_stats(col(\"value\")).alias(\"smoothed\"))\n",
    "result.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cb11e-eb06-47bd-b794-8d17eb6f199a",
   "metadata": {},
   "source": [
    "### 6. Сравнение производительности SCALAR UDF и SCALAR_ITER UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf84ffef-b025-4cc1-b39b-2294fc791355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCALAR UDF time: 0.12 seconds\n",
      "SCALAR_ITER UDF time: 0.12 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Обычный SCALAR UDF\n",
    "@pandas_udf(\"double\")\n",
    "def scalar_square(series: pd.Series) -> pd.Series:\n",
    "    return series ** 2\n",
    "\n",
    "# SCALAR_ITER UDF\n",
    "@pandas_udf(\"double\")\n",
    "def scalar_iter_square(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for batch in iterator:\n",
    "        yield batch ** 2\n",
    "\n",
    "# Большой датасет\n",
    "large_data = [(i, float(i)) for i in range(1, 200001)]\n",
    "large_df = spark.createDataFrame(large_data, [\"id\", \"value\"])\n",
    "\n",
    "# Тестирование SCALAR\n",
    "start = time.time()\n",
    "result1 = large_df.select(\"id\", scalar_square(col(\"value\")).alias(\"squared\"))\n",
    "count1 = result1.count()\n",
    "scalar_time = time.time() - start\n",
    "\n",
    "# Тестирование SCALAR_ITER\n",
    "start = time.time()\n",
    "result2 = large_df.select(\"id\", scalar_iter_square(col(\"value\")).alias(\"squared\"))\n",
    "count2 = result2.count()\n",
    "scalar_iter_time = time.time() - start\n",
    "\n",
    "print(f\"SCALAR UDF time: {scalar_time:.2f} seconds\")\n",
    "print(f\"SCALAR_ITER UDF time: {scalar_iter_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff492b5a-6d36-486f-9bc8-60e6fb6cce0b",
   "metadata": {},
   "source": [
    "### 5. Обработка ошибок в SCALAR и SCALAR_ITER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "51582bd4-af6d-47c4-be4d-f3d4a7ec4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALAR - ошибка влияет на всю partition\n",
    "@pandas_udf(\"double\")\n",
    "def scalar_errorprocessing(series: pd.Series) -> pd.Series:\n",
    "    # Если ошибка - вся partition падает\n",
    "    return series.apply(lambda x: 1/x if x != 0 else float('inf'))\n",
    "\n",
    "# SCALARITER - можно обработать ошибки по батчам\n",
    "@pandas_udf(\"double\")\n",
    "def scalar_iter_errorprocessing(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for batchnum, batch in enumerate(iterator):\n",
    "        try:\n",
    "            result = batch.apply(lambda x: 1/x if x != 0 else float('inf'))\n",
    "            yield result\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_num}: {e}\")\n",
    "            # Возвращаем безопасные значения\n",
    "            yield pd.Series([0.0] * len(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0323d-804e-4a6d-b04f-4277ac00228e",
   "metadata": {},
   "source": [
    "## SCALAR_ITER используется если:\n",
    "\n",
    "1. **Большие датасеты** - если нужно контролировать использование памяти\n",
    "2. **Stateful обработка** -  если нужно сохранять состояние между батчами\n",
    "3. **Инициализация ресурсов** -  если дорого создавать объекты для каждого вызова\n",
    "4. **Кэширование** - если можно переиспользовать вычисления\n",
    "5. **Потоковая обработка** - если данные обрабатываются как поток\n",
    "\n",
    "⚠️ **SCALAR** может вызвать OutOfMemory на больших данных  \n",
    "\n",
    "✅ **SCALAR_ITER** обработает по частям"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c9e13f-50e3-4f33-9c0d-cf99d1f65077",
   "metadata": {},
   "source": [
    "## Сравнительная таблица:\n",
    "| Характеристика | SCALAR | SCALAR_ITER |\n",
    "|---------------|---------|-------------|\n",
    "| **Память** | Загружает всю partition | Обрабатывает батчами |\n",
    "| **Состояние** | Не сохраняется | Сохраняется между батчами |\n",
    "| **Инициализация** | При каждом вызове | Один раз на partition |\n",
    "| **Производительность** | Быстрее для малых данных | Лучше для больших данных |\n",
    "| **Сложность** | Проще в реализации | Требует понимания итераторов |\n",
    "| **Кэширование** | Ограниченное | Эффективное |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "64ddfa3d-cad7-46d9-9018-64d8bc8a5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948806e-3cc9-4b85-b718-3d017b12455e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
