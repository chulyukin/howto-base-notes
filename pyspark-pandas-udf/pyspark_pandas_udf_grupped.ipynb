{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6b9979-d088-487d-9843-b54ccf2b6ac4",
   "metadata": {},
   "source": [
    "# PySpark - Pandas UDF. Групповые типы функций \n",
    "`(применение через .applyInPandas, pandas_udf для GROUPED_AGG) + скалярные mapInPandas (+ .cogroup)`  \n",
    "\n",
    "В данном Notebook рассматриваются типы функций: \n",
    " - применение через .applyInPandas (ранняя реализация GROUPED_MAP)\n",
    " - GROUPED_AGG  **.mapInPandas**, **.cogroup.applyInPandas**\n",
    "\n",
    "| Тип  | Скалярные/Групповые | Возвращаемое значение | Трансформация | Реализация |\n",
    "|:-----|:-------|:--------|:---------------|:---------------|\n",
    "| **GROUPED_MAP** (устаревшее) | Групповые | DataFrame | Сложная обработка групп | .applyInPandas(func, schema)\n",
    "| **GROUPED_AGG** | Групповые | Scalar | Агрегация групп |  @pandas_udf(type) |\n",
    "| Нет | Скалярные             | Iterator | Обработка партиций | .mapInPandas(func, schema)  |\n",
    "| Нет | Скалярные             | Scalar | Обработка двух групп | .cogroup().applyInPandas() |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ab3e8-a4ed-488d-bebc-2d301774bc5d",
   "metadata": {},
   "source": [
    "### Описание  \n",
    "```python\n",
    "pyspark.sql.functions.pandas_udf (f=None, returnType=None, functionType=None)\n",
    "#    f=None,            - Функция для преобразования в UDF\n",
    "#    returnType=None,   - Тип возвращаемого значения\n",
    "#    functionType=None  - Тип UDF (depricted)\n",
    "```\n",
    "**Тип функций:** *GROUPED_MAP*, *GROUPED_AGG*\n",
    "\n",
    "### Параметр `returnType` (Строковые обозначения):\n",
    "```python\n",
    "# для .applyInPandas(func, schema)\n",
    "pandas.DataFrame # согласно схеме вывода pyspark.sql.types.StructType\n",
    "\n",
    "# для GROUPED_AGG (в т.ч с .cogroup())\n",
    "@pandas_udf(<\"int\", \"long\", \"float\", \"double\", \"string\", \"boolean\", \"date\", \"timestamp\">)\n",
    "\n",
    "# для .mapInPandas(func, schema)\n",
    "pandas.DataFrame # согласно схеме вывода pyspark.sql.types.StructType\n",
    "```\n",
    "### Возвращаемое значение  \n",
    "```python\n",
    "pandas.DataFrame # согласно схеме вывода pyspark.sql.types.StructType\n",
    "```\n",
    "\n",
    "Параметр, устанавливающий максимальное количество записей в одном **Arrow batch** при передаче данных между Spark и Python процессами:\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"<Количество строк в батче>\")\n",
    "```\n",
    "см. https://spark.apache.org/docs/3.5.7/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a18c73-d947-4fe4-b15a-cefcb91390ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/11 14:20:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"python\")\n",
    "os.environ[\"SPARK_LOCAL_IP\"]='localhost'\n",
    "from pyspark import SparkContext, SparkConf#, HiveContext\n",
    "conf = SparkConf()\\\n",
    "             .setAppName(\"Example Spark\")\\\n",
    "             .setMaster(\"local[3]\")\\\n",
    "             .setAppName(\"CountingSheep\")\\\n",
    "             .set(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4330b6e6-aa4c-4f9d-89c2-85a63f76fa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.13.7 (main, Nov 24 2025 20:51:28)\n",
      "Spark context Web UI available at http://localhost:4040\n",
      "Spark context available as 'sc' (master = local[3], app id = local-1768130438853).\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8b8d3e4-8bee-4b7b-98c2-59791ac98c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[3]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CountingSheep</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7cf36397e120>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "707fe64d-03fc-4d44-bcb9-b019892126fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f_\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, pandas_udf, rand\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d0dce-75df-4278-84c3-4560ca4f1a13",
   "metadata": {},
   "source": [
    "## Предварительная подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ccc75-bf1c-46ad-b8cc-87f63db41f41",
   "metadata": {},
   "source": [
    "### DataSet Titanic\n",
    "| Колонка  | Тип | Описание| Комментарий |\n",
    "|:-----|:-------|:--------|:---------------|\n",
    "| PassengerId | integer | Уникальный идентификатор пассажира | Уникальный ключ | \n",
    "| Survived | integer | Выжившие | 1 - пассажир выжил, 0 - погиб (целевая перемнная) | \n",
    "| Pclass | string | Класс билета пассажира | Значения 1 - 3   | \n",
    "| Name | string | Фамилия и имя пассажира | Пример значения: Braund, Mr. Owen Harris  | \n",
    "| Sex | string | Пол пассажира | male - мужской, female - женский  | \n",
    "| Age | double | Возраст пассажира | Измеряется в годах  | \n",
    "| SibSp | integer | Количество братьев, сестер, сводных братьев, сводных сестер, супругов на борту | Число от 0 до 8 | \n",
    "| Parch | integer | Количество родителей, детей (в том числе приемных) на борту | Число от 0 до 9 | \n",
    "| Ticket | string | Номер билета пассажира| Пример значения: STON/O2. 3101282 | \n",
    "| Fare | double | Плата за проезд| Пример значения: 133.65 | \n",
    "| Cabin | string | Каюта| Пример значения: C123 | \n",
    "| Embarked| string |  Код порта посадки пассажира | S — Саутгемптон; C — Шербур; Q — Квинстаун. (NULL означает Саутгемптон) | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01f3c40-e7fb-45b2-9f11-752ae7fa854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train и test\n",
    "trainfile = \"./Data/train.csv\"\n",
    "testfile  = \"./Data/test.csv\"\n",
    "\n",
    "titanic_train_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(trainfile)\n",
    "titanic_test_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(testfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d738ce-403b-486b-b729-f29bbcf7e80d",
   "metadata": {},
   "source": [
    "### Тестовый random DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6983430d-1ebe-4bcd-a9d3-df6813649c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "nRow = 0\n",
    "for product in [\"String_A\", \"String_B\"]:\n",
    "    for i in range(15):\n",
    "        nRow += 1\n",
    "        date = datetime(2014, 5, 2) + timedelta(days=i*3)\n",
    "        value1 = round(.55 + i * 1.08 + (0.68 if product == \"String_A\" else .23),6)\n",
    "        value2 = 110.0 + i * 5 + (10 if product == \"String_A\" else 20)\n",
    "        timestamp_val = datetime(2007, 5, 8, 16, 35) + timedelta(days=i*5*.05 + i*9)\n",
    "        data.append((nRow, product, date, value1,value2, timestamp_val))\n",
    "\n",
    "dfData = spark.createDataFrame(data, [\"rowid\",\"str_value\", \"date_val\", \"first_double_val\", \"second_double_val\",\"timestamp_val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332a6938-d3ba-4a57-a50e-08d0b2e49030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------------------+----------------+-----------------+-------------------+\n",
      "|rowid|str_value|date_val           |first_double_val|second_double_val|timestamp_val      |\n",
      "+-----+---------+-------------------+----------------+-----------------+-------------------+\n",
      "|1    |String_A |2014-05-02 00:00:00|1.23            |120.0            |2007-05-08 16:35:00|\n",
      "|2    |String_A |2014-05-05 00:00:00|2.31            |125.0            |2007-05-17 22:35:00|\n",
      "|3    |String_A |2014-05-08 00:00:00|3.39            |130.0            |2007-05-27 04:35:00|\n",
      "|4    |String_A |2014-05-11 00:00:00|4.47            |135.0            |2007-06-05 10:35:00|\n",
      "|5    |String_A |2014-05-14 00:00:00|5.55            |140.0            |2007-06-14 16:35:00|\n",
      "|6    |String_A |2014-05-17 00:00:00|6.63            |145.0            |2007-06-23 22:35:00|\n",
      "|7    |String_A |2014-05-20 00:00:00|7.71            |150.0            |2007-07-03 04:35:00|\n",
      "|8    |String_A |2014-05-23 00:00:00|8.79            |155.0            |2007-07-12 10:35:00|\n",
      "+-----+---------+-------------------+----------------+-----------------+-------------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfData.show(8,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d154ce8-b961-4a5d-a968-f2667e1c0971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow batch size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Arrow batch size:\", spark.conf.get(\"spark.sql.execution.arrow.maxRecordsPerBatch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137f5ee-faf9-415a-a123-6a5d8500305c",
   "metadata": {},
   "source": [
    "## Example .applyInPandas ( устар. PANDAS_UDF.GROUPED_MAP)  \n",
    "\n",
    "Использование **applyInPandas** в PySpark имеет смысл в сценариях применения пользовательских функций к сгруппированным данным с использованием возможностей Pandas\n",
    "\n",
    "**Особенности применения applyInPandas:**\n",
    "\n",
    "1. **Работа с Pandas DataFrame** - группа данных передается в функцию как Pandas DataFrame, что позволяет использовать возможности Pandas для обработки данных\n",
    "2. **Схема вывода** - необходимо явно определять схему вывода с помощью StructType\n",
    "3. **Высокая производительность** - Pandas UDF может повысить производительность по сравнению с классическими UDF (данные обрабатываются в пакетах и используют возможности векторизованных операций Pandas)\n",
    "4. **Обработка по группам** - данные автоматически группируются по указанному столбцу, и функция применяется отдельно к каждой группе\n",
    "5. **Векторизация операций** - операции производятся над целыми колонками данных, что значительно ускоряет их выполнение по сравнению с классическими циклическими подходами\n",
    "   \n",
    "**Пример определения:**\n",
    "```python\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pandas as pd\n",
    "# устаревший синтаксис @pandas_udf(IntegerType(), PandasUDFType.GROUPED_MAP)\n",
    "\n",
    "# Определяется схема вывода\n",
    "output_schema = StructType([\n",
    "    StructField(\"gruoped_key\", StringType(), True),\n",
    "    StructField(\"agg_value\", DoubleType(), True)\n",
    "                    ])\n",
    "\n",
    "# Определяется функция для применения\n",
    "def calculate_fuction(pdf: pd.DataFrame):\n",
    "    return pd.DataFrame({\n",
    "        \"key\": [pdf['date_value'].iloc[0]],\n",
    "        \"value1_avg\": [pdf['double_value1'].mean()]               \n",
    "                       })\n",
    "\n",
    "# Прменение средствами .applyInPandas\n",
    "dfData.groupby(\"gruoped_key\").applyInPandas(calculate_fuction, schema=output_schema).show()\n",
    "\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78639797-b671-49d1-ac0b-6029e42ba417",
   "metadata": {},
   "source": [
    "**applyInPandas** используется, если:  \n",
    "- необходимо применять сложные функции к каждой группе данных (например пользовательские агрегаты, фильтрации или трансформации, трудно реализуемыми стандартными функциями Spark) \n",
    "- логику обработки данных эффективнее реализовать с использованием Pandas\n",
    "- требуется воспользоваться векторизованными операциями Pandas, которые могут быть более быстрыми чем операции в Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79a7d40c-9962-493b-ac9e-a8cde4b348c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyArrow version: 22.0.0\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "print(f\"PyArrow version: {pa.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78ccf9-07be-429e-b1c7-960e4dc862e1",
   "metadata": {},
   "source": [
    "### 1. Простая функция pandas_udf с applyInPandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9d130-85d6-4755-a386-99926636ce24",
   "metadata": {},
   "source": [
    "**Базовый пример**  \n",
    "Простой пример для иллюстрации обработки и применения applyInPandas    \n",
    "*Подсчет avg, max, min по группе*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff1fbe9-4ad6-441a-ac3d-553f0781ef55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------+---------+\n",
      "|                key|value_avg|value_min|value_max|\n",
      "+-------------------+---------+---------+---------+\n",
      "|2014-05-02 00:00:00|    1.005|    120.0|    130.0|\n",
      "|2014-05-05 00:00:00|    2.085|    125.0|    135.0|\n",
      "|2014-05-08 00:00:00|    3.165|    130.0|    140.0|\n",
      "|2014-05-11 00:00:00|    4.245|    135.0|    145.0|\n",
      "|2014-05-14 00:00:00|    5.325|    140.0|    150.0|\n",
      "|2014-05-17 00:00:00|    6.405|    145.0|    155.0|\n",
      "|2014-05-20 00:00:00|    7.485|    150.0|    160.0|\n",
      "|2014-05-23 00:00:00|    8.565|    155.0|    165.0|\n",
      "|2014-05-26 00:00:00|    9.645|    160.0|    170.0|\n",
      "|2014-05-29 00:00:00|   10.725|    165.0|    175.0|\n",
      "+-------------------+---------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Определение схемы вывода*\n",
    "output_schema = StructType([\n",
    "    StructField(\"key\", StringType(), True),\n",
    "    StructField(\"value_avg\", DoubleType(), True),\n",
    "    StructField(\"value_min\", DoubleType(), True),\n",
    "    StructField(\"value_max\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Определение функции для применения\n",
    "def calculate_agg(pdf: pd.DataFrame):\n",
    "    \"\"\"Среднее, минимальное и максимальное внутри группы\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        \"key\": [pdf['date_val'].iloc[0]],\n",
    "        \"value_avg\": [pdf['value_for_avg'].mean().round(6)],\n",
    "        \"value_min\": [pdf['value_for_minmax'].min()],\n",
    "        \"value_max\": [pdf['value_for_minmax'].max()]\n",
    "                       })\n",
    "    \n",
    "# Применение функции с помощью applyInPandas\n",
    "dfData.select(f_.col(\"date_val\").cast(\"String\")\n",
    "             ,f_.col(\"first_double_val\").alias(\"value_for_avg\")\n",
    "             ,f_.col(\"second_double_val\").alias(\"value_for_minmax\")) \\\n",
    "       .groupby(\"date_val\") \\\n",
    "       .applyInPandas(calculate_agg, schema=output_schema) \\\n",
    "       .show(10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f0c51-2495-4d16-a917-3d6b50d11c03",
   "metadata": {},
   "source": [
    "**Нормализация и ранжирование**\n",
    "Еще один пример реализации внутренней обработки с вызовом методов **pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65998d26-8380-4c5e-b010-f5346fcb6963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----+--------------+----+\n",
      "|     key|           date_val|value|normalized_val|rank|\n",
      "+--------+-------------------+-----+--------------+----+\n",
      "|String_A|2014-06-13 00:00:00|  190|           1.0|   1|\n",
      "|String_B|2014-06-13 00:00:00|  200|           1.0|   1|\n",
      "|String_A|2014-06-10 00:00:00|  185|      0.928571|   2|\n",
      "|String_B|2014-06-10 00:00:00|  195|      0.928571|   2|\n",
      "|String_A|2014-06-07 00:00:00|  180|      0.857143|   3|\n",
      "|String_B|2014-06-07 00:00:00|  190|      0.857143|   3|\n",
      "|String_A|2014-06-04 00:00:00|  175|      0.785714|   4|\n",
      "|String_B|2014-06-04 00:00:00|  185|      0.785714|   4|\n",
      "|String_A|2014-06-01 00:00:00|  170|      0.714286|   5|\n",
      "|String_B|2014-06-01 00:00:00|  180|      0.714286|   5|\n",
      "+--------+-------------------+-----+--------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Определение схемы вывода\n",
    "output_schema = StructType([\n",
    "    StructField(\"key\", StringType()),\n",
    "    StructField(\"date_val\", StringType()),\n",
    "    StructField(\"value\", IntegerType()),\n",
    "    StructField(\"normalized_val\", DoubleType()),\n",
    "    StructField(\"rank\", IntegerType())\n",
    "])\n",
    "\n",
    "# Функция обработки группы\n",
    "def normalize_and_rank(pdf: pd.DataFrame):\n",
    "    \"\"\"Нормализация и ранжирование внутри группы\"\"\"\n",
    "    # Нормализация (0-1)\n",
    "    min_val = pdf['value'].min()\n",
    "    max_val = pdf['value'].max()\n",
    "    \n",
    "    if max_val > min_val:\n",
    "        pdf['normalized_val'] = round((pdf['value'] - min_val) / (max_val - min_val),6)\n",
    "    else:\n",
    "        pdf['normalized_val'] = 0.0\n",
    "    \n",
    "    # Ранжирование\n",
    "    pdf['rank'] = pdf['value'].rank(ascending=False, method='dense').astype(int)\n",
    "    \n",
    "    return pdf\n",
    "\n",
    "# Применение функции с помощью applyInPandas\n",
    "dfData.select(f_.col(\"str_value\").alias(\"key\")\n",
    "             ,f_.col(\"date_val\").cast(\"String\")\n",
    "             ,f_.col(\"second_double_val\").alias(\"value\")) \\\n",
    "      .groupBy(\"key\").applyInPandas(normalize_and_rank, schema=output_schema)\\\n",
    "      .orderBy(\"rank\")\\\n",
    "      .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d716e6-73d3-4997-b7d9-fc02babbb5d0",
   "metadata": {},
   "source": [
    "### 2. Интерполяция пропущенных значений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d7fb72-4d01-4187-94f9-21ab00d4b087",
   "metadata": {},
   "source": [
    "**Исходные данные**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "835c739c-e6fc-4b50-8a18-7f0e0e5d07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age is null: 177\n"
     ]
    }
   ],
   "source": [
    "df_interp = (titanic_train_csv\n",
    "                .select( f_.col(\"Sex\").cast(\"string\").alias(\"series\")\n",
    "                        ,f_.col(\"PassengerId\").alias(\"index\")\n",
    "                        ,f_.col(\"Age\").alias(\"value\") \n",
    "                       ))   \n",
    "\n",
    "print(\"Age is null:\", titanic_train_csv.where(\"Age is null\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08241d7-ff5f-4310-908c-453da9222529",
   "metadata": {},
   "source": [
    "**Функция pandas_udf для применения**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bbf8f2c-24b1-4f46-8f17-34936805bf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age is null: 0\n",
      "+------+-----+-----+------------+-----------+-----------+\n",
      "|series|index|value|value_linear|value_ffill|value_bfill|\n",
      "+------+-----+-----+------------+-----------+-----------+\n",
      "|female|    2| 38.0|        38.0|       38.0|       38.0|\n",
      "|female|    3| 26.0|        26.0|       26.0|       26.0|\n",
      "|female|    4| 35.0|        35.0|       35.0|       35.0|\n",
      "|female|    9| 27.0|        27.0|       27.0|       27.0|\n",
      "|female|   10| 14.0|        14.0|       14.0|       14.0|\n",
      "|female|   11|  4.0|         4.0|        4.0|        4.0|\n",
      "|female|   12| 58.0|        58.0|       58.0|       58.0|\n",
      "|female|   15| 14.0|        14.0|       14.0|       14.0|\n",
      "|female|   16| 55.0|        55.0|       55.0|       55.0|\n",
      "|female|   19| 31.0|        31.0|       31.0|       31.0|\n",
      "+------+-----+-----+------------+-----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Исходные данные\n",
    "df_interp = (titanic_train_csv\n",
    "                .select( f_.col(\"Sex\").cast(\"string\").alias(\"series\")\n",
    "                        ,f_.col(\"PassengerId\").alias(\"index\")\n",
    "                        ,f_.col(\"Age\").alias(\"value\") \n",
    "                       ))   \n",
    "\n",
    "titanic_train_csv.where(\"Age is null\").count()\n",
    "\n",
    "# Определение схемы вывода\n",
    "schema_interp = StructType([\n",
    "    StructField(\"series\", StringType()),\n",
    "    StructField(\"index\", IntegerType()),\n",
    "    StructField(\"value\", DoubleType()),\n",
    "    StructField(\"value_linear\", DoubleType()),\n",
    "    StructField(\"value_ffill\", DoubleType()),\n",
    "    StructField(\"value_bfill\", DoubleType())\n",
    "])\n",
    "\n",
    "# Функция обработки\n",
    "def interpolate_values(pdf):\n",
    "    \"\"\"Различные методы интерполяции пропущенных значений\"\"\"\n",
    "    pdf = pdf.sort_values('index').reset_index(drop=True)\n",
    "    # Линейная интерполяция\n",
    "    pdf['value_linear'] = pdf['value'].interpolate(method='linear')\n",
    "    # Forward fill\n",
    "    pdf['value_ffill'] = pdf['value'].ffill()\n",
    "    # Backward fill\n",
    "    pdf['value_bfill'] = pdf['value'].bfill()\n",
    "    \n",
    "    return pdf\n",
    "\n",
    "# Применение функции\n",
    "result_interp = df_interp.groupBy(\"series\").applyInPandas(interpolate_values, schema=schema_interp)\n",
    "\n",
    "print(\"Age is null:\", result_interp.where(\"value_linear is null\").count())\n",
    "\n",
    "result_interp.orderBy(\"series\", \"index\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace58452-b7d8-43d5-bc48-e2b9de0aa335",
   "metadata": {},
   "source": [
    "### 3. Машинное обучение по группам\n",
    "На примере DataSet \"Titanic\" (Логистическая регрессия - задача классификации):\n",
    "\n",
    "**Задача:** Классификация (**Модель**:Логистическая регрессия)\n",
    "\n",
    "**Dataset:** Titanic (tran: train.csv, test: test.csv)\n",
    "  \n",
    "**Таргет:** \"Survived\" (1 - выжившие)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe89e5-e6f0-481d-896a-9a054ceeb18c",
   "metadata": {},
   "source": [
    "### Целевая переменная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06f428cd-2cd1-49e0-b7e5-b89934bc5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = \"Survived\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3edd6-4695-41dd-9953-6675c52b32db",
   "metadata": {},
   "source": [
    "### DataSet Titanic - очистка данных и создание дополнительных фичей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ba2b1-3348-4c0b-86dc-24583838290f",
   "metadata": {},
   "source": [
    "**Функция подсчета простых статистик по DataSet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4e7870d-949e-4c1e-b339-60f152b1ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleDataStat(full: DataFrame, train: DataFrame, test: DataFrame):\n",
    "    \"\"\" \n",
    "    Функция простой статистики DataSet для train, test и полного DataSet.\n",
    "    Выводит на экран рассчитанные показатели count с учетом целевой переменной\n",
    "    \"\"\"\n",
    "    \n",
    "    full_cnt = full.count(); train_cnt = train.count(); test_cnt = test.count()\n",
    "    \n",
    "    print(\"Count data: FULL: \", full_cnt, f\"({full_cnt/full_cnt*100}%)\"\n",
    "                      ,\"TRAIN:\", train_cnt, f\"({round(train_cnt/full_cnt*100,2)}%)\"\n",
    "                      ,\"TEST:\", test_cnt, f\"({round(test_cnt/full_cnt*100,2)}%)\")    \n",
    "    full.groupBy(f_.lit(\"full\").alias(\"dataset\"),\"label\").count()\\\n",
    "        .unionByName(train.groupBy(\"label\",f_.lit(\"train\").alias(\"dataset\")).count())\\\n",
    "        .unionByName(test.groupBy(\"label\",f_.lit(\"test\").alias(\"dataset\")).count())\\\n",
    "        .groupBy(\"label\").pivot(\"dataset\").max(\"count\")\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563d518-aa91-47fe-bf09-f51e76f73654",
   "metadata": {},
   "source": [
    "**Очистка и фичи**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73d9fa4b-5476-4222-87de-0514c6ed1c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count data: FULL:  1309 (100.0%) TRAIN: 891 (68.07%) TEST: 418 (31.93%)\n",
      "+-----+----+----+-----+\n",
      "|label|full|test|train|\n",
      "+-----+----+----+-----+\n",
      "|    1| 494| 152|  342|\n",
      "|    0| 815| 266|  549|\n",
      "+-----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mapping титулов в поле name\n",
    "titul_mapping = {\n",
    "   \"Mlle\": \"Miss\", \"Ms\":  \"Miss\", \"Dona\":  \"Mrs\", \"Mme\":  \"Mrs\", \"Lady\": \"Mrs\", \"Countess\": \"Mrs\"\n",
    "  ,\"Rev\":  \"Mr\",   \"Col\": \"Mr\",   \"Major\": \"Mr\",  'Capt': \"Mr\"\n",
    "  ,\"Jonkheer\": \"Sir\", \"Don\":  \"Sir\"\n",
    "  ,\"Dr\":   \"Rare\"}\n",
    "\n",
    "# дополнительные фичи\n",
    "ft_extra = [f_.when(f_.col(\"Age\")<=15.0,15.0)\\\n",
    "              .when(f_.col(\"Age\").isNull(),28)\\\n",
    "              .when(f_.col(\"Age\") < 25,25)\\\n",
    "              .when(f_.col(\"Age\") <= 34,34)\\\n",
    "              .when(f_.col(\"Age\") <= 44,44).otherwise(55).alias(\"ft_cat_age\")\n",
    "      ,f_.regexp_extract(f_.col(\"Name\"), r\" ([A-Za-z]+)\\.\", 1).alias(\"ft_titul\")\n",
    "      ,(f_.col(\"SibSp\") +f_.col(\"Parch\") + 1).alias(\"ft_family_size\")\n",
    "      ,f_.substring(f_.col(\"Cabin\"),1,1).alias(\"ft_class_cabin\") ]\n",
    "\n",
    "# Преобразование DataSet\n",
    "traindf = titanic_train_csv.select(*titanic_train_csv.columns, *ft_extra).withColumnRenamed(label_col,\"label\")\n",
    "testdf = titanic_test_csv.select(*titanic_test_csv.columns, *ft_extra).withColumnRenamed(label_col,\"label\")\n",
    "titanicdf = traindf.unionByName(testdf)\n",
    "\n",
    "# Статистика\n",
    "simpleDataStat(full=titanicdf, train=traindf, test=testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b371b6f0-886c-4faf-9cde-b5e89afbc921",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03d3d648-c58d-4a6c-b786-97749d9285af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f3e75-f571-4c57-8b42-7a4b44c17724",
   "metadata": {},
   "source": [
    "**Метрики ACCURACY, PRECISION, RECALL, F1:**  \n",
    "Расчетные значения:\n",
    "- *TP* $(True Positive)$ — верно предсказанные положительные\n",
    "- *TN* $(True Negative)$ — верно предсказанные отрицательные\n",
    "- *FP* $(False Positive)$ — ложные срабатывания\n",
    "- *FN* $(False Negative)$ — пропуски предсказания\n",
    "\n",
    "Метрики:  \n",
    "\n",
    "$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $  \n",
    "\n",
    "$ Precision = \\frac{TP}{TP + FP} $  \n",
    "\n",
    "$ Recall = \\frac{TP}{TP + FN} $\n",
    "\n",
    "$ F1 = \\frac{2TP}{2TP + FP + FN} $ или ($ F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall} $)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60fad6ef-72da-448b-a1e5-f27b3ee14dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metric(df: DataFrame, group_column = f_.lit(1), info = None):\n",
    "    \"\"\" \n",
    "    Функция расчета метрик ACCURACY, PRECISION, RECALL, F1.\n",
    "    Возвращает DataFrame\n",
    "    \"\"\"    \n",
    "    return df.groupBy(f_.lit(info).alias(\"info\"), group_column)\\\n",
    "             .agg(\n",
    "                 f_.sum(f_.col(\"prediction\") * f_.col(\"label\")).alias(\"TP\")\n",
    "                ,f_.sum(((f_.col(\"prediction\")==0)&(f_.col(\"label\")==0)).cast(\"integer\")).alias(\"TN\") \n",
    "                ,f_.sum(((f_.col(\"prediction\")==1)&(f_.col(\"label\")==0)).cast(\"integer\")).alias(\"FP\") \n",
    "                ,f_.sum(((f_.col(\"prediction\")==0)&(f_.col(\"label\")==1)).cast(\"integer\")).alias(\"FN\") \n",
    "                )\\\n",
    "            .select(\"info\",\"TP\", \"TN\", \"FP\", \"FN\" \n",
    "                   ,f_.expr(\"(TP+TN)/(TP+TN+FP+FN)\").alias(\"ACCURACY\")\n",
    "                   ,f_.expr(\"TP/(TP+FP)\").alias(\"PRECISION\")        \n",
    "                   ,f_.expr(\"TP/(TP+FN)\").alias(\"RECALL\")\n",
    "                   ,f_.expr(\"2*TP/(2*TP+FP+FN)\").alias(\"F1\")\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3db5af-1378-4fcf-8107-64963d817eaa",
   "metadata": {},
   "source": [
    "**Метрики ROC AUC и GINI** \n",
    "\n",
    "Реализованы средствами **pyspark.ml**  \n",
    "\n",
    "$ ROC AUC $ расчитывается через BinaryClassificationEvaluator  \n",
    "\n",
    "$ GINI = 2 \\cdot roc_auc - 1 $  \n",
    "\n",
    "**Пример:**\n",
    "```python\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# DataSet после применения модели\n",
    "predictions_df = df_result_train.select(\"label\",\"probability\")\n",
    "\n",
    "# Оценьщик\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    rawPredictionCol=\"rawPrediction\",  # или probability, в зависимости от модели\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "roc_auc = evaluator.evaluate(predictions_df)\n",
    "print(roc_auc,  2*roc_auc -1)\n",
    "```\n",
    "\n",
    "Где `predictions_df` — DataFrame с колонками:\n",
    "   - `label` — истинный класс (0/1),\n",
    "   - `rawPrediction` — выход модели (`probability`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd17e3a-6664-4d07-83b3-472ffa7671f2",
   "metadata": {},
   "source": [
    "**evaluator для ROC AUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9c27bea-56f0-465f-8c86-68e17792c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    rawPredictionCol=\"probability\", \n",
    "    metricName=\"areaUnderROC\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960bde3b-1256-48a8-ba87-1a2f33da2234",
   "metadata": {},
   "source": [
    "**pandas_udf функция обучения модели логистической регрессии** (для применение через applyInPandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f355870e-5259-498f-802b-b02dacfd79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выходная схема модели обучения\n",
    "modelschema = StructType([StructField(\"group_key\", StringType(), False), StructField(\"model_bytes\", BinaryType(), False)])\n",
    "\n",
    "# Функция обучения\n",
    "def train_model(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Функция (pandas_udf) тренировки модели LogisticRegression. см.*\n",
    "    Колонки \n",
    "        label - целевая переменная (с классми 1 или 0)\n",
    "        group_key - колонка группировки. см.** \n",
    "    \n",
    "    Внешние переменные:\n",
    "        numfeatures  - числовые фичи\n",
    "        catfeatures  - категориальные фичи\n",
    "       \n",
    "    Параметры:\n",
    "        - pdf – тренировочный DataSet (train как pandas.DataFrame)\n",
    "        \n",
    "    Return: DataFrame с бинарными строками сериализованной модели.\n",
    "            Согласно схеме: StructField(\"group_key\", StringType()), StructField(\"model_bytes\", BinaryType())\n",
    "\n",
    "    *вызов через .applyInPandas\n",
    "    **в случае нескольких групп - будут обучены несколько моделей (одна на каждую группу)       \n",
    "    \"\"\"\n",
    "    # Если нет колонки group_key - обучение по всему DataSet\n",
    "    pdf['group_key'] = pdf.get('group_key', \"NO GROUP KEY\")\n",
    "    # Все фичи\n",
    "    feature_cols =  numfeatures + catfeatures\n",
    "    # Конвейер: импьютация чисел + OHE категорий + LogisticRegression\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", SimpleImputer(strategy=\"median\"), numfeatures),\n",
    "            (\"cat\", Pipeline([\n",
    "                (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "            ]), catfeatures)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", pre),\n",
    "        (\"clf\", LogisticRegression(max_iter=400, solver=\"liblinear\"))\n",
    "    ])\n",
    "\n",
    "    # Фильтр строк без таргета\n",
    "    pdf = pdf.dropna(subset=[\"label\"])\n",
    "\n",
    "    X = pdf[feature_cols]\n",
    "    y = pdf[\"label\"].astype(int)\n",
    "    \n",
    "    # Обучение\n",
    "    pipe.fit(X, y)\n",
    "\n",
    "    #Группа\n",
    "    print(\"GROUP:\", [pdf['group_key'].iloc[0]])\n",
    "    # Коэффициенты\n",
    "    print(\"coef:\", pipe[-1].coef_)\n",
    "    print(\"bias:\", pipe[-1].intercept_)\n",
    "    print(\"=================================================\")    \n",
    "    \n",
    "    # Модель\n",
    "    return pd.DataFrame({\n",
    "        \"group_key\": [pdf['group_key'].iloc[0]]\n",
    "       ,\"model_bytes\": [pickle.dumps(pipe)]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a0488-2dcd-4e30-8d41-9d23cc535b69",
   "metadata": {},
   "source": [
    "**Функция применения модели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5ad04c9-d9cc-44d8-94f1-f928e856b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Schena\n",
    "pred_schema = StructType([\n",
    "    StructField(\"PassengerId\", IntegerType(), False),\n",
    "    StructField(\"probability\", DoubleType(), False),\n",
    "    StructField(\"prediction\", IntegerType(), False), ])\n",
    "\n",
    "# Функция применения модели\n",
    "def inference_model(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Функция (pandas_udf) применения модели\n",
    "    Колонка probability - колонка с рассчитанной вероятностью\n",
    "    Колонка prediction - колонка предсказания (1,0 согласно trashhold (переменная pred) )\n",
    "\n",
    "    Внешние переменные:\n",
    "        numfeatures  - числовые фичи\n",
    "        catfeatures  - категориальные фичи\n",
    "     \n",
    "    Параметры:\n",
    "        - pdf – DataSet для inference\n",
    "        \n",
    "    Return: DataFrame с бинарными строками сериализованной модели.\n",
    "            Согласно схеме:     \n",
    "                StructField(\"PassengerId\", IntegerType()) - идентификатор строки\n",
    "                StructField(\"probability\", DoubleType()) - вероятность\n",
    "                StructField(\"prediction\", IntegerType()) - предсказание\n",
    "    \"\"\"    \n",
    "    pipe = pickle.loads(pdf[\"model_bytes\"].iloc[0])\n",
    "    X = pdf[list(numfeatures + catfeatures)]\n",
    "    proba = pipe.predict_proba(X)[:, 1]\n",
    "    pred = (proba >= 0.50).astype(int)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"PassengerId\": pdf[\"PassengerId\"].astype(int).values,\n",
    "        \"probability\": proba,\n",
    "        \"prediction\": pred\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d1c9a2-fdf8-48b2-be84-7b5f7151dd29",
   "metadata": {},
   "source": [
    "**Отобранные фичи**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2118f1b-58d0-4f18-a201-93d1b53b78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "numfeatures = [\"ft_family_size\",\"Fare\",\"Pclass\"] # Числовые фичи\n",
    "catfeatures = [\"SibSp\",\"Parch\", \"Sex\",\"ft_titul\",\"ft_class_cabin\",\"ft_cat_age\" ] # Категориальные фичи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209eb2be-5dca-489b-9f61-d930f59e1cb4",
   "metadata": {},
   "source": [
    "**Тренировка модели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9606e0ec-0865-42c1-b3e1-39a7205e07e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GROUP: ['NO GROUP KEY']=============================>           (158 + 3) / 200]\n",
      "coef: [[-0.43665192  0.00563984 -0.63743176  0.47946149  0.69793652  0.97560474\n",
      "  -0.42705458 -0.19187886 -0.42390711 -0.10807391  0.10920399  0.23404936\n",
      "   0.49504337  0.72617025 -0.48881061  0.06107622 -0.13464429  1.43087475\n",
      "  -0.42878646 -0.31381195  0.14790219  0.0433201  -0.37754905 -0.22544841\n",
      "  -0.34812029  0.15898499  0.10493882  2.03784885  0.09183242  0.14790572\n",
      "   0.0651606  -0.95787665  0.64695358  0.16676653 -0.83626821  0.44954904\n",
      "   0.13145218  0.1618445  -0.12132718  0.80073114  0.98443457  0.2261824\n",
      "  -0.59978954 -0.18985501 -0.39158477  0.85598794  0.09643507  0.14657049\n",
      "   0.45380304  0.03227922 -0.58298746]]\n",
      "bias: [1.00208829]\n",
      "=================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------------------------------------+\n",
      "|   group_key|                                            model_bytes|\n",
      "+------------+-------------------------------------------------------+\n",
      "|NO GROUP KEY|[80 04 95 7B 0D 00 00 00 00 00 00 8C 10 73 6B 6C 65 ...|\n",
      "+------------+-------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "modeldf = (\n",
    "    traindf\n",
    "        .select(*(numfeatures + catfeatures), \"label\")\n",
    "        .groupBy(f_.lit(\"1\").alias(\"dummy\"))      # одна группа = весь датасет\n",
    "        .applyInPandas(train_model, schema=modelschema)\n",
    ").cache()\n",
    "# modeldf содержит строку с бинарной моделью\n",
    "modeldf.show(truncate=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e208d9-d6fe-449f-9f29-c6df9f9e8c5f",
   "metadata": {},
   "source": [
    "### Применение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3baa0-7554-42a3-a4fa-a364f3006ca8",
   "metadata": {},
   "source": [
    "**Применение модели к train DataSet через applyInPandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adcb1acb-7f15-483f-87ca-50765ab88101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------+-----+------+---------------+------+----+-----+-----+----------+-------+-----+--------+----------+--------+--------------+--------------+------------+---------------+\n",
      "|PassengerId|    probability|prediction|label|Pclass|           Name|   Sex| Age|SibSp|Parch|    Ticket|   Fare|Cabin|Embarked|ft_cat_age|ft_titul|ft_family_size|ft_class_cabin|   group_key|    model_bytes|\n",
      "+-----------+---------------+----------+-----+------+---------------+------+----+-----+-----+----------+-------+-----+--------+----------+--------+--------------+--------------+------------+---------------+\n",
      "|        148|0.6360269871...|         1|    0|     3|\"Ford, Miss....|female| 9.0|    2|    2|W./C. 6608| 34.375| NULL|       S|      15.0|    Miss|             5|          NULL|NO GROUP KEY|[80 04 95 7B...|\n",
      "|        463|0.4374759515...|         0|    0|     1|Gee, Mr. Art...|  male|47.0|    0|    0|    111320|   38.5|  E63|       S|      55.0|      Mr|             1|             E|NO GROUP KEY|[80 04 95 7B...|\n",
      "|        471|0.0871467225...|         0|    0|     3|Keefe, Mr. A...|  male|NULL|    0|    0|    323592|   7.25| NULL|       S|      28.0|      Mr|             1|          NULL|NO GROUP KEY|[80 04 95 7B...|\n",
      "|        496|0.0904355550...|         0|    0|     3|Yousseff, Mr...|  male|NULL|    0|    0|      2627|14.4583| NULL|       C|      28.0|      Mr|             1|          NULL|NO GROUP KEY|[80 04 95 7B...|\n",
      "|        833|0.0871373908...|         0|    0|     3| Saad, Mr. Amin|  male|NULL|    0|    0|      2671| 7.2292| NULL|       C|      28.0|      Mr|             1|          NULL|NO GROUP KEY|[80 04 95 7B...|\n",
      "+-----------+---------------+----------+-----+------+---------------+------+----+-----+-----+----------+-------+-----+--------+----------+--------+--------------+--------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_with_model = traindf.crossJoin(modeldf)\n",
    "pred_train_df = (train_with_model\n",
    "                    .groupBy(f_.lit(1).alias(\"dummy\"))  # одной группой, как и при обучении\n",
    "                    .applyInPandas(inference_model, schema=pred_schema)\n",
    "                )\n",
    "df_result_train = pred_train_df.join(train_with_model, [\"PassengerId\"])\n",
    "\n",
    "df_result_train.show(5, truncate=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302128d3-10aa-4dfd-96a8-dc9449a50448",
   "metadata": {},
   "source": [
    "**Применение модели к test DataSet через applyInPandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f32ad20d-ea68-44f6-bc5a-b217e4f774d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------+------+---------------+------+----+-----+-----+-------+-------+-----+--------+-----+----------+--------+--------------+--------------+------------+---------------+\n",
      "|PassengerId|    probability|prediction|Pclass|           Name|   Sex| Age|SibSp|Parch| Ticket|   Fare|Cabin|Embarked|label|ft_cat_age|ft_titul|ft_family_size|ft_class_cabin|   group_key|    model_bytes|\n",
      "+-----------+---------------+----------+------+---------------+------+----+-----+-----+-------+-------+-----+--------+-----+----------+--------+--------------+--------------+------------+---------------+\n",
      "|        892|0.0787098527...|         0|     3|Kelly, Mr. J...|  male|34.5|    0|    0| 330911| 7.8292| NULL|       Q|    0|      44.0|      Mr|             1|          NULL|NO GROUP KEY|[80 04 95 7B...|\n",
      "|        893|0.5414954950...|         1|     3|Wilkes, Mrs....|female|47.0|    1|    0| 363272|    7.0| NULL|       S|    1|      55.0|     Mrs|             2|          NULL|NO GROUP KEY|[80 04 95 7B...|\n",
      "|        894|0.0811099311...|         0|     2|Myles, Mr. T...|  male|62.0|    0|    0| 240276| 9.6875| NULL|       Q|    0|      55.0|      Mr|             1|          NULL|NO GROUP KEY|[80 04 95 7B...|\n",
      "|        895|0.1157012843...|         0|     3|Wirz, Mr. Al...|  male|27.0|    0|    0| 315154| 8.6625| NULL|       S|    0|      34.0|      Mr|             1|          NULL|NO GROUP KEY|[80 04 95 7B...|\n",
      "|        896|0.6373316179...|         1|     3|Hirvonen, Mr...|female|22.0|    1|    1|3101298|12.2875| NULL|       S|    1|      25.0|     Mrs|             3|          NULL|NO GROUP KEY|[80 04 95 7B...|\n",
      "+-----------+---------------+----------+------+---------------+------+----+-----+-----+-------+-------+-----+--------+-----+----------+--------+--------------+--------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Для каждой строки необходимо добавить колонку с моделью\n",
    "test_with_model = testdf.crossJoin(modeldf)\n",
    "\n",
    "pred_test_df = (\n",
    "    test_with_model\n",
    "        .groupBy(f_.lit(1).alias(\"dummy\"))  # одной группой, как и при обучении\n",
    "        .applyInPandas(inference_model, schema=pred_schema)\n",
    ")\n",
    "df_result_test = pred_test_df.join(test_with_model, [\"PassengerId\"])\n",
    "\n",
    "df_result_test.show(5, truncate=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe506dd-2a6b-4b59-93d0-e18c765fb0c3",
   "metadata": {},
   "source": [
    "### Расчет метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2f0ccf6-3779-49e3-aa95-34a34eba8a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:>                (0 + 1) / 1][Stage 111:>                (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+---+------------------+------------------+------------------+------------------+\n",
      "|info| TP| TN| FP| FN|          ACCURACY|         PRECISION|            RECALL|                F1|\n",
      "+----+---+---+---+---+------------------+------------------+------------------+------------------+\n",
      "|TRAN|265|485| 64| 77|0.8417508417508418|0.8054711246200608|0.7748538011695907| 0.789865871833085|\n",
      "|TEST|147|241| 25|  5|0.9282296650717703|0.8546511627906976|0.9671052631578947|0.9074074074074074|\n",
      "+----+---+---+---+---+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "calc_metric(df=df_result_train, info=\"TRAN\").unionByName(calc_metric(df=df_result_test,info = \"TEST\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7472c7b2-6b91-4601-9d58-b4f3c5671f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: ROC AUC: 0.8844656419433531 GINI: 0.7689312838867062\n",
      "test: ROC AUC: 0.9469232291254451 GINI: 0.8938464582508903\n"
     ]
    }
   ],
   "source": [
    "roc_auc_train = evaluator.evaluate(df_result_train.select(\"label\",\"probability\"))\n",
    "roc_auc_test = evaluator.evaluate( df_result_test.select(\"label\",\"probability\"))\n",
    "\n",
    "print(\"train: ROC AUC:\",roc_auc_train, \"GINI:\", 2* roc_auc_train -1)\n",
    "print(\"test: ROC AUC:\",roc_auc_test, \"GINI:\", 2* roc_auc_test -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1400580a-941e-4004-a0ca-70c42ba0ad86",
   "metadata": {},
   "source": [
    "### Групповая модель (одна модель на группу)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e536e52b-53c7-4a44-8ef5-f26b6b208cfc",
   "metadata": {},
   "source": [
    "**Разбиение DataSet на train и test с учетом группировки по PClass**  \n",
    "\n",
    "Класс каюты:  \n",
    " - 1 — Первый;\n",
    " - 2 — Второй;\n",
    " - 3 — Третий\n",
    "\n",
    "Строится 3 модели выживаемости в каждом классе каюты, для каждого класса  по одной\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28b3d626-4917-49a4-9bc1-d43b7c455c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_strata = titanicdf.where(\"pclass is not null\") \\\n",
    "                          .withColumn(\"strata\",f_.concat_ws(\"_\", f_.col(\"pclass\"), f_.col(\"label\").cast(\"string\")))\n",
    "\n",
    "test_fraction = 0.25\n",
    "\n",
    "# одна и та же доля для всех страт\n",
    "strata_values = [row[\"strata\"] for row in df_with_strata.select(\"strata\").distinct().collect()]\n",
    "fractions = {s: test_fraction for s in strata_values}\n",
    "\n",
    "group_testdf = df_with_strata.stat.sampleBy(\"strata\", fractions, seed=42).drop(\"strata\")\n",
    "\n",
    "group_traindf = df_with_strata.join(group_testdf.select(\"PassengerId\"), on=\"PassengerId\", how=\"left_anti\").drop(\"strata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9993d2c-94f0-4ea8-af68-447e873af45f",
   "metadata": {},
   "source": [
    "**Статистика**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b7e227e-8d82-4acc-b7da-338efde945aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PClass: 1\n",
      "Count data: FULL:  323 (100.0%) TRAIN: 236 (73.07%) TEST: 87 (26.93%)\n",
      "+-----+----+----+-----+\n",
      "|label|full|test|train|\n",
      "+-----+----+----+-----+\n",
      "|    1| 186|  52|  134|\n",
      "|    0| 137|  35|  102|\n",
      "+-----+----+----+-----+\n",
      "\n",
      "PClass: 2\n",
      "Count data: FULL:  277 (100.0%) TRAIN: 213 (76.9%) TEST: 64 (23.1%)\n",
      "+-----+----+----+-----+\n",
      "|label|full|test|train|\n",
      "+-----+----+----+-----+\n",
      "|    1| 117|  34|   83|\n",
      "|    0| 160|  30|  130|\n",
      "+-----+----+----+-----+\n",
      "\n",
      "PClass: 3\n",
      "Count data: FULL:  709 (100.0%) TRAIN: 526 (74.19%) TEST: 183 (25.81%)\n",
      "+-----+----+----+-----+\n",
      "|label|full|test|train|\n",
      "+-----+----+----+-----+\n",
      "|    1| 191|  55|  136|\n",
      "|    0| 518| 128|  390|\n",
      "+-----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pclass in [\"1\", \"2\", \"3\"]: \n",
    "    print(\"PClass:\", pclass)\n",
    "    simpleDataStat(full=titanicdf.where(f\"PClass = '{pclass}'\")\n",
    "                  ,train=group_traindf.where(f\"PClass = '{pclass}'\")\n",
    "                  ,test=group_testdf.where(f\"PClass = '{pclass}'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f7b3c-ef3a-42e9-9d5a-058011af26e3",
   "metadata": {},
   "source": [
    "**Отобранные фичи**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0cebf6e-40cd-43bb-b811-06d4b2599cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numfeatures = [\"Age\", \"ft_family_size\", \"Fare\"] # Числовые Фичи\n",
    "catfeatures = [\"Sex\",\"SibSp\",\"Parch\"] # Категориальные Фичи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03536cb-6ffb-4675-9279-bedd735d34e5",
   "metadata": {},
   "source": [
    "**Тренировка модели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5608ecb4-015d-4cbb-ade4-31e28e672903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GROUP: [np.int32(2)]\n",
      "coef: [[-0.04671291  0.54301482 -0.01535855  2.33338555 -2.00327483  0.58300092\n",
      "  -0.11560798 -0.18177719  0.04449498 -0.19973869  0.51365234  0.00366198\n",
      "   0.01253509]]\n",
      "bias: [0.33011072]\n",
      "=================================================\n",
      "GROUP: [np.int32(3)]\n",
      "coef: [[-0.02745026 -0.33495111  0.01365803  1.60498287 -1.34907181  0.13533635\n",
      "   0.40602761  0.7768091  -1.04849935  0.23602821  0.14073219 -0.39052305\n",
      "  -0.35157364  0.19066159 -0.04132843 -0.05619206  0.35764523  0.19209625\n",
      "  -0.01983033 -0.01556757]]\n",
      "bias: [0.25591106]\n",
      "=================================================\n",
      "GROUP: [np.int32(1)]===========================================>(199 + 1) / 200]\n",
      "coef: [[-0.02644635  0.33416936 -0.00265923  2.38591004 -1.80073118  0.21935206\n",
      "  -0.01591867  0.35516746  0.02657801  0.8357008   0.11601491 -0.10303421\n",
      "  -0.11890431 -0.14459834]]\n",
      "bias: [0.58517886]\n",
      "=================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------------------------------------+\n",
      "|group_key|                                            model_bytes|\n",
      "+---------+-------------------------------------------------------+\n",
      "|        1|[80 04 95 A8 0A 00 00 00 00 00 00 8C 10 73 6B 6C 65 ...|\n",
      "|        3|[80 04 95 F3 0A 00 00 00 00 00 00 8C 10 73 6B 6C 65 ...|\n",
      "|        2|[80 04 95 AD 0A 00 00 00 00 00 00 8C 10 73 6B 6C 65 ...|\n",
      "+---------+-------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "group_modelschema = StructType([StructField(\"group_key\", IntegerType(), False), StructField(\"model_bytes\", BinaryType(), False)])\n",
    "# Train\n",
    "group_modeldf = (\n",
    "    group_traindf.where(\"pclass is not null\")\n",
    "                 .withColumn(\"group_key\",f_.col(\"pclass\"))\n",
    "                 .select(*(numfeatures+catfeatures), \"label\",\"group_key\")\n",
    "                 .groupBy(f_.col(\"group_key\").alias(\"group_key\"))      # по группам pclass\n",
    "                 .applyInPandas(train_model, schema=group_modelschema)).cache()\n",
    "\n",
    "# modeldf содержит строку с бинарной моделью\n",
    "group_modeldf.show(truncate=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10b96f-d7db-40f6-bff0-c5109e1de174",
   "metadata": {},
   "source": [
    "### Применение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f09776e-a065-4aae-bbbb-5778e8ccebb0",
   "metadata": {},
   "source": [
    "**Применение модели к train DataSet через applyInPandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87c3a247-965c-4b0b-82ae-007587936e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------+-----+------+---------------+------+----+-----+-----+-----------+------+-----+--------+----------+--------+--------------+--------------+---------+---------------+\n",
      "|PassengerId|    probability|prediction|label|Pclass|           Name|   Sex| Age|SibSp|Parch|     Ticket|  Fare|Cabin|Embarked|ft_cat_age|ft_titul|ft_family_size|ft_class_cabin|group_key|    model_bytes|\n",
      "+-----------+---------------+----------+-----+------+---------------+------+----+-----+-----+-----------+------+-----+--------+----------+--------+--------------+--------------+---------+---------------+\n",
      "|        463|0.0825133947...|         0|    0|     1|Gee, Mr. Art...|  male|47.0|    0|    0|     111320|  38.5|  E63|       S|      55.0|      Mr|             1|             E|        1|[80 04 95 A8...|\n",
      "|        833|0.0993543809...|         0|    0|     3| Saad, Mr. Amin|  male|NULL|    0|    0|       2671|7.2292| NULL|       C|      28.0|      Mr|             1|          NULL|        3|[80 04 95 F3...|\n",
      "|        243|0.0913719385...|         0|    0|     2|Coleridge, M...|  male|29.0|    0|    0|W./C. 14263|  10.5| NULL|       S|      34.0|      Mr|             1|          NULL|        2|[80 04 95 AD...|\n",
      "|        392|0.1077118634...|         0|    1|     3|Jansson, Mr....|  male|21.0|    0|    0|     350034|7.7958| NULL|       S|      25.0|      Mr|             1|          NULL|        3|[80 04 95 F3...|\n",
      "|        540|0.9669607960...|         1|    1|     1|Frolicher, M...|female|22.0|    0|    2|      13568|  49.5|  B39|       C|      25.0|    Miss|             3|             B|        1|[80 04 95 A8...|\n",
      "+-----------+---------------+----------+-----+------+---------------+------+----+-----+-----+-----------+------+-----+--------+----------+--------+--------------+--------------+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_w_group_model = group_traindf.join(group_modeldf,[f_.col(\"group_key\")==f_.col(\"pclass\")])\n",
    "\n",
    "pred_group_train_df = (\n",
    "    train_w_group_model\n",
    "        .groupBy(f_.col(\"Sex\").alias(\"group_key\"))  # по группам pclass, как и при обучении\n",
    "        .applyInPandas(inference_model, schema=pred_schema))\n",
    "\n",
    "df_result_group_train = pred_group_train_df.join(train_w_group_model, [\"PassengerId\"])\n",
    "\n",
    "df_result_group_train.show(5, truncate=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e21ff-f812-43c9-9a43-39e43bd680de",
   "metadata": {},
   "source": [
    "**Применение модели к test DataSet через applyInPandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa7c79ab-89c5-409b-9ad6-d21d1dd38df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------+-----+------+---------------+------+----+-----+-----+----------+-------+-----+--------+----------+--------+--------------+--------------+---------+---------------+\n",
      "|PassengerId|    probability|prediction|label|Pclass|           Name|   Sex| Age|SibSp|Parch|    Ticket|   Fare|Cabin|Embarked|ft_cat_age|ft_titul|ft_family_size|ft_class_cabin|group_key|    model_bytes|\n",
      "+-----------+---------------+----------+-----+------+---------------+------+----+-----+-----+----------+-------+-----+--------+----------+--------+--------------+--------------+---------+---------------+\n",
      "|        148|0.7584245442...|         1|    0|     3|\"Ford, Miss....|female| 9.0|    2|    2|W./C. 6608| 34.375| NULL|       S|      15.0|    Miss|             5|          NULL|        3|[80 04 95 F3...|\n",
      "|        471|0.0993798048...|         0|    0|     3|Keefe, Mr. A...|  male|NULL|    0|    0|    323592|   7.25| NULL|       S|      28.0|      Mr|             1|          NULL|        3|[80 04 95 F3...|\n",
      "|        496|0.1085456368...|         0|    0|     3|Yousseff, Mr...|  male|NULL|    0|    0|      2627|14.4583| NULL|       C|      28.0|      Mr|             1|          NULL|        3|[80 04 95 F3...|\n",
      "|        858|0.0640623626...|         0|    1|     1|Daly, Mr. Pe...|  male|51.0|    0|    0|    113055|  26.55|  E17|       S|      55.0|      Mr|             1|             E|        1|[80 04 95 A8...|\n",
      "|        251|0.0993798048...|         0|    0|     3|Reed, Mr. Ja...|  male|NULL|    0|    0|    362316|   7.25| NULL|       S|      28.0|      Mr|             1|          NULL|        3|[80 04 95 F3...|\n",
      "+-----------+---------------+----------+-----+------+---------------+------+----+-----+-----+----------+-------+-----+--------+----------+--------+--------------+--------------+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_w_group_model = group_testdf.join(group_modeldf,[f_.col(\"group_key\")==f_.col(\"pclass\")])\n",
    "\n",
    "pred_group_test_df = (\n",
    "    test_w_group_model\n",
    "        .groupBy(f_.col(\"Sex\").alias(\"group_key\"))  # одной группой, как и при обучении\n",
    "        .applyInPandas(inference_model, schema=pred_schema))\n",
    "\n",
    "df_result_group_test = pred_group_test_df.join(test_w_group_model, [\"PassengerId\"])\n",
    "\n",
    "df_result_group_test.show(5, truncate=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbcef63-2834-4eda-a529-8c9221c31255",
   "metadata": {},
   "source": [
    "### Расчет метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b85c968-33a0-46a8-ba4e-fe6c49b1e05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pclass: 1\n",
      "+----+---+---+---+---+------------------+------------------+------------------+------------------+\n",
      "|info| TP| TN| FP| FN|          ACCURACY|         PRECISION|            RECALL|                F1|\n",
      "+----+---+---+---+---+------------------+------------------+------------------+------------------+\n",
      "|TRAN|102| 91| 11| 32|0.8177966101694916|0.9026548672566371|0.7611940298507462|0.8259109311740891|\n",
      "|TEST| 41| 35|  0| 11|0.8735632183908046|               1.0|0.7884615384615384|0.8817204301075269|\n",
      "+----+---+---+---+---+------------------+------------------+------------------+------------------+\n",
      "\n",
      "pclass: 2\n",
      "+----+---+---+---+---+------------------+---------+------------------+------------------+\n",
      "|info| TP| TN| FP| FN|          ACCURACY|PRECISION|            RECALL|                F1|\n",
      "+----+---+---+---+---+------------------+---------+------------------+------------------+\n",
      "|TRAN| 69|124|  6| 14|0.9061032863849765|     0.92|0.8313253012048193|0.8734177215189873|\n",
      "|TEST| 30| 30|  0|  4|            0.9375|      1.0|0.8823529411764706|            0.9375|\n",
      "+----+---+---+---+---+------------------+---------+------------------+------------------+\n",
      "\n",
      "pclass: 3\n",
      "+----+---+---+---+---+------------------+------------------+------------------+------------------+\n",
      "|info| TP| TN| FP| FN|          ACCURACY|         PRECISION|            RECALL|                F1|\n",
      "+----+---+---+---+---+------------------+------------------+------------------+------------------+\n",
      "|TRAN|102|334| 56| 34|0.8288973384030418|0.6455696202531646|              0.75|0.6938775510204082|\n",
      "|TEST| 40|117| 11| 15|0.8579234972677595|0.7843137254901961|0.7272727272727273|0.7547169811320755|\n",
      "+----+---+---+---+---+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pclass in [\"1\", \"2\", \"3\"]:\n",
    "    print(\"pclass:\", pclass)\n",
    "    calc_metric(df=df_result_group_train.where(f\"Pclass = '{pclass}'\"), info=\"TRAN\")\\\n",
    "        .unionByName(calc_metric(df=df_result_group_test.where(f\"Pclass = '{pclass}'\"),info = \"TEST\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "701b198b-4ae7-4c42-9519-bd60d070a6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass = 1\n",
      "train: ROC AUC: 0.8798713235294117 GINI: 0.7597426470588233\n",
      "test:  ROC AUC: 0.9666666666666667 GINI: 0.9333333333333333\n",
      "Pclass = 2\n",
      "train: ROC AUC: 0.9262945846664299 GINI: 0.8525891693328598\n",
      "test:  ROC AUC: 0.9814814814814815 GINI: 0.962962962962963\n",
      "Pclass = 3\n",
      "train: ROC AUC: 0.8048816300713839 GINI: 0.6097632601427678\n",
      "test:  ROC AUC: 0.9083904109589045 GINI: 0.816780821917809\n"
     ]
    }
   ],
   "source": [
    "for pclass in [\"1\", \"2\", \"3\"]:\n",
    "    predictions_train = df_result_train.where(f\"Pclass = '{pclass}'\").select(\"label\",\"probability\")\n",
    "    predictions_test = df_result_test.where(f\"Pclass = '{pclass}'\").select(\"label\",\"probability\")\n",
    "\n",
    "    roc_auc_train = evaluator.evaluate(predictions_train)\n",
    "    roc_auc_test = evaluator.evaluate(predictions_test)\n",
    "    print(f\"Pclass = {pclass}\")\n",
    "    print(f\"train: ROC AUC: {roc_auc_train}\", \"GINI:\", 2* roc_auc_train -1)\n",
    "    print(f\"test:  ROC AUC:\",roc_auc_test,  \"GINI:\", 2* roc_auc_test -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd24dbb-c4bc-482c-8662-393a6d63d995",
   "metadata": {},
   "source": [
    "### 4. Создание фичей для временных рядов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba8b99-3e68-4e6c-a11a-624f5fd0be92",
   "metadata": {},
   "source": [
    "**Исходные данные**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdcdb576-f08f-4835-8bd9-163ada2a4081",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "# Данные продаж по часам\n",
    "hourly_data = []\n",
    "for store in [\"Store_A\", \"Store_B\"]:\n",
    "    base_sales = 100 if store == \"Store_A\" else 150\n",
    "    for hour in range(0, 24 * 7, 2):  # Неделя данных\n",
    "        timestamp = datetime(2024, 1, 1) + timedelta(hours=hour)\n",
    "        \n",
    "        # Имитируем паттерны: пик днем, спад ночью\n",
    "        hour_of_day = timestamp.hour\n",
    "        day_pattern = 1.5 if 9 <= hour_of_day <= 18 else 0.7\n",
    "        \n",
    "        # Выходные имеют другой паттерн\n",
    "        weekend_pattern = 1.2 if timestamp.weekday() >= 5 else 1.0\n",
    "        \n",
    "        sales = round(base_sales * day_pattern * weekend_pattern + np.random.randn() * 10,4)\n",
    "        hourly_data.append((store, timestamp, float(sales)))\n",
    "\n",
    "df_hourly = spark.createDataFrame(hourly_data, [\"store\", \"timestamp\", \"sales\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8babc-c189-4444-97c1-2a81f07f5f3a",
   "metadata": {},
   "source": [
    "**Схема результата**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e748fe4-a679-40cb-982e-8034eeb92770",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_features = StructType([\n",
    "    StructField(\"store\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"sales\", DoubleType()),\n",
    "    # Временные признаки\n",
    "    StructField(\"hour\", IntegerType()),\n",
    "    StructField(\"weekday\", IntegerType()),\n",
    "    StructField(\"is_weekend\", IntegerType()),\n",
    "    StructField(\"mon_day\", IntegerType()),\n",
    "    # Лаговые признаки\n",
    "    StructField(\"lag_24h\", DoubleType()),\n",
    "    # Скользящие статистики\n",
    "    StructField(\"roll_mean_24h\", DoubleType()),\n",
    "    StructField(\"roll_std_24h\", DoubleType()),\n",
    "    StructField(\"roll_min_24h\", DoubleType()),\n",
    "    StructField(\"roll_max_24h\", DoubleType()),\n",
    "    # Экспоненциальное скользящее среднее\n",
    "    StructField(\"ema_12h\", DoubleType()),\n",
    "    StructField(\"ema_24h\", DoubleType()),\n",
    "    # Изменения\n",
    "    StructField(\"diff_2h\", DoubleType()),\n",
    "    StructField(\"pct_chng_1h\", DoubleType()),\n",
    "    StructField(\"pct_chng_24h\", DoubleType()),\n",
    "    # Кумулятивные признаки\n",
    "    StructField(\"cumsum\", DoubleType()),\n",
    "    StructField(\"cummax\", DoubleType()),\n",
    "    StructField(\"cummin\", DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c26a6d-e8fa-4276-8850-43aba4e50f0c",
   "metadata": {},
   "source": [
    "**Функция для применения через .applyInPandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0aeff953-8235-448e-b182-22620f8c0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timeseries_features(pdf):\n",
    "    \"\"\"Создание признаков для временных рядов\"\"\"\n",
    "    # Сортируем по времени\n",
    "    pdf = pdf.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # ===== ВРЕМЕННЫЕ ПРИЗНАКИ =====\n",
    "    pdf['hour'] = pdf['timestamp'].dt.hour\n",
    "    pdf['weekday'] = pdf['timestamp'].dt.dayofweek\n",
    "    pdf['is_weekend'] = (pdf['weekday'] >= 5).astype(int)\n",
    "    pdf['mon_day'] = pdf['timestamp'].dt.day\n",
    "    \n",
    "    # ===== ЛАГОВЫЕ ПРИЗНАКИ =====\n",
    "    pdf['lag_24h'] = pdf['sales'].shift(24)\n",
    "    \n",
    "    # ===== СКОЛЬЗЯЩИЕ СТАТИСТИКИ =====\n",
    "    # Скользящее среднее\n",
    "    pdf['roll_mean_24h'] = round(pdf['sales'].rolling(window=24, min_periods=1).mean(),10)\n",
    "    \n",
    "    # Скользящее стандартное отклонение\n",
    "    pdf['roll_std_24h'] = round(pdf['sales'].rolling(window=24, min_periods=1).std(),10)\n",
    "    \n",
    "    # Скользящие min/max\n",
    "    pdf['roll_min_24h'] = round(pdf['sales'].rolling(window=24, min_periods=1).min(),10)\n",
    "    pdf['roll_max_24h'] = round(pdf['sales'].rolling(window=24, min_periods=1).max(),10)\n",
    "    \n",
    "    # ===== ЭКСПОНЕНЦИАЛЬНОЕ СКОЛЬЗЯЩЕЕ СРЕДНЕЕ =====\n",
    "    pdf['ema_12h'] = round(pdf['sales'].ewm(span=12, adjust=False).mean(),10)\n",
    "    pdf['ema_24h'] = round(pdf['sales'].ewm(span=24, adjust=False).mean(),10)\n",
    "    \n",
    "    # ===== ИЗМЕНЕНИЯ =====\n",
    "    # Абсолютное изменение\n",
    "    pdf['diff_2h'] = round(pdf['sales'].diff(2),3)\n",
    "    \n",
    "    # Процентное изменение\n",
    "    pdf['pct_chng_1h'] = round(pdf['sales'].pct_change(1) * 100,3)\n",
    "    pdf['pct_chng_24h'] = round(pdf['sales'].pct_change(24) * 100,3)\n",
    "    \n",
    "    # ===== КУМУЛЯТИВНЫЕ ПРИЗНАКИ =====\n",
    "    pdf['cumsum'] = round(pdf['sales'].cumsum(),3)\n",
    "    pdf['cummax'] = round(pdf['sales'].cummax(),3)\n",
    "    pdf['cummin'] = round(pdf['sales'].cummin(),3)\n",
    "    \n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14393650-69dc-4120-9c3d-71e3376cf85e",
   "metadata": {},
   "source": [
    "**Применение через .applyInPandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e454d26-ee58-416c-a54d-9312b726c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+----+-------+----------+-------+-------+--------------+-------------+------------+------------+--------------+--------------+-------+-----------+------------+--------+-------+------+\n",
      "|  store| timestamp|   sales|hour|weekday|is_weekend|mon_day|lag_24h| roll_mean_24h| roll_std_24h|roll_min_24h|roll_max_24h|       ema_12h|       ema_24h|diff_2h|pct_chng_1h|pct_chng_24h|  cumsum| cummax|cummin|\n",
      "+-------+----------+--------+----+-------+----------+-------+-------+--------------+-------------+------------+------------+--------------+--------------+-------+-----------+------------+--------+-------+------+\n",
      "|Store_A|2024-01-01| 74.9671|   0|      0|         0|      1|   NULL|       74.9671|         NULL|     74.9671|     74.9671|       74.9671|       74.9671|   NULL|       NULL|        NULL|  74.967| 74.967|74.967|\n",
      "|Store_A|2024-01-01| 68.6174|   2|      0|         0|      1|   NULL|      71.79225| 4.4899159285|     68.6174|     74.9671| 73.9902230769|     74.459124|   NULL|      -8.47|        NULL| 143.584| 74.967|68.617|\n",
      "|Store_A|2024-01-01| 76.4769|   4|      0|         0|      1|   NULL|       73.3538| 4.1707298798|     68.6174|     76.4769| 74.3727887574|   74.62054608|   1.51|     11.454|        NULL| 220.061| 76.477|68.617|\n",
      "|Store_A|2024-01-01| 85.2303|   6|      0|         0|      1|   NULL|     76.322925| 6.8453978372|     68.6174|     85.2303| 76.0431751024| 75.4693263936| 16.613|     11.446|        NULL| 305.292|  85.23|68.617|\n",
      "|Store_A|2024-01-01| 67.6585|   8|      0|         0|      1|   NULL|      74.59004| 7.0823058235|     67.6585|     85.2303| 74.7532250867| 74.8444602821| -8.818|    -20.617|        NULL|  372.95|  85.23|67.658|\n",
      "|Store_A|2024-01-01|147.6586|  10|      0|         0|      1|   NULL| 86.7681333333|30.4952945228|     67.6585|    147.6586| 85.9694366118| 80.6695914595| 62.428|    118.241|        NULL| 520.609|147.659|67.658|\n",
      "|Store_A|2024-01-01|165.7921|  12|      0|         0|      1|   NULL| 98.0572714286|40.8299109628|     67.6585|    165.7921| 98.2498463638| 87.4793921428| 98.134|     12.281|        NULL| 686.401|165.792|67.658|\n",
      "|Store_A|2024-01-01|157.6743|  14|      0|         0|      1|   NULL|      105.5094|43.2804906711|     67.6585|    165.7921|107.3920700002| 93.0949847714| 10.016|     -4.896|        NULL| 844.075|165.792|67.658|\n",
      "|Store_A|2024-01-01|145.3053|  16|      0|         0|      1|   NULL|109.9311666667|42.6030391842|     67.6585|    165.7921|113.2248746155| 97.2718099896|-20.487|     -7.845|        NULL|  989.38|165.792|67.658|\n",
      "|Store_A|2024-01-01|155.4256|  18|      0|         0|      1|   NULL|     114.48061|42.6652613046|     67.6585|    165.7921|119.7172939054|101.9241131905| -2.249|      6.965|        NULL|1144.806|165.792|67.658|\n",
      "+-------+----------+--------+----+-------+----------+-------+-------+--------------+-------------+------------+------------+--------------+--------------+-------+-----------+------------+--------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_features = df_hourly.groupBy(\"store\").applyInPandas(\n",
    "    create_timeseries_features, \n",
    "    schema=schema_features\n",
    ")\n",
    "\n",
    "result_features.orderBy(\"store\", \"timestamp\") \\\n",
    ".withColumn(\"timestamp\", f_.col(\"timestamp\").cast(\"date\"))\\\n",
    ".show(10, truncate=15, vertical = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a7ceb-35e2-400e-8f88-b8c6518db0c5",
   "metadata": {},
   "source": [
    "**Статистика по признакам**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3e73cdc-8ab2-43a7-9e1a-2e85844bd1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------------+------------------+------------------+------------------+\n",
      "|summary|  store|            sales|     roll_mean_24h|           ema_24h|      pct_chng_24h|\n",
      "+-------+-------+-----------------+------------------+------------------+------------------+\n",
      "|  count|    168|              168|               168|               168|               120|\n",
      "|   mean|   NULL|136.1564898809523|128.90968077511852|126.31511784454047|10.186474999999998|\n",
      "| stddev|   NULL|63.21476668527007|29.668561025060946|30.947864059345694|19.182998324611788|\n",
      "|    min|Store_A|          50.4033|          71.79225|         74.459124|           -27.747|\n",
      "|    max|Store_B|          281.586|          188.1172|    199.9060811578|            90.275|\n",
      "+-------+-------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_features.select(\n",
    "    \"store\", \"sales\", \"roll_mean_24h\", \"ema_24h\", \"pct_chng_24h\"\n",
    ").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4739e177-19bc-47e8-9b3c-dd9cb74819de",
   "metadata": {},
   "source": [
    "## 5. Вызов pandas_udf с дополнительными параметрами (на примере расчета PSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f10cb-68fa-42e8-b5e8-2ba6ba95ad07",
   "metadata": {},
   "source": [
    "**PSI - Population Stability Index**  \n",
    "Метрика для определения изменения структуры (распределения) признака или скоринговой модели между двумя выборками/периодами.\n",
    "\n",
    "$ PSI = \\sum_{i=1}^{n} (actual_i - expected_i) \\times \\ln\\left(\\frac{actual_i}{expected_i}\\right) $\n",
    "\n",
    "где:  \n",
    "-  $actual_i$ - доля наблюдений в $i$-й группе в (бине) текущей выборке\n",
    "-  $expected_i$ - доля наблюдений в $i$-й группе (бине) в базовой выборке\n",
    "-  $n$ - количество бинов\n",
    "\n",
    "**Интерпретация:**  \n",
    "\n",
    "- **PSI < 0.1** – данные стабильны\n",
    "- **0.1 ≤ PSI < 0.25** – умеренный сдвиг, нужен мониторинг или настройка\n",
    "- **PSI ≥ 0.25** – сильный сдвиг, требуется разбор причин, возможено переобучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea8d26-ef00-4846-aa49-c5a15be680d4",
   "metadata": {},
   "source": [
    "**Исходные данные**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0e9b489-1b45-49d1-9c64-e2174afb0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (0, 0.43, 0.38, 0.43), (0, 0.36, 0.43, 0.52), (0, 0.53, 0.56, 0.65), (0, 0.56, 0.60, 0.55), (0, 0.59, 0.62, 0.56),\n",
    "    (1, 0.42, 0.30, 0.38), (1, 0.35, 0.37, 0.47), (1, 0.36, 0.45, 0.56), (1, 0.51, 0.55, 0.61), (1, 0.52, 0.56, 0.61),\n",
    "    (2, 0.36, 0.47, 0.59), (2, 0.34, 0.33, 0.65), (2, 0.35, 0.43, 0.68), (2, 0.36, 0.45, 0.69),\n",
    "    (3, 0.72, 0.71, 0.89), (3, 0.56, 0.63, 0.87), (3, 0.15, 0.28, 0.63)\n",
    "]\n",
    "columns = [\"period_no\", \"value_1\", \"value_2\", \"value_3\"]\n",
    "df_for_psi = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1fb62-5d73-4e4e-9f7c-4825a2d3eae5",
   "metadata": {},
   "source": [
    "**Функция для применения через applyInPandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e3fc481-a533-4f30-acdb-ac9ecfe5e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psi(pdf: pd.DataFrame, period_col='period_no', actual_period_value=0, bins=9) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Рассчитывает PSI для каждой переменной (value_1, value_2, value_3)\n",
    "    относительно актуального периода (period_no =  0)\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_psi(actual, expected):\n",
    "        \"\"\"\n",
    "        Вычисляет PSI между двумя распределениями\n",
    "        \"\"\"\n",
    "        # Удаляем пропуски\n",
    "        actual_clean = actual.dropna()\n",
    "        expected_clean = expected.dropna()\n",
    "        \n",
    "        if len(actual_clean) == 0 or len(expected_clean) == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        # Определяем границы бинов на основе актуального периода\n",
    "        _, bin_edges = np.histogram(actual_clean, bins=bins)\n",
    "        \n",
    "        # Распределяем данные по бинам\n",
    "        actual_counts, _ = np.histogram(actual_clean, bins=bin_edges)\n",
    "        expected_counts, _ = np.histogram(expected_clean, bins=bin_edges)\n",
    "        \n",
    "        # Вычисляем пропорции (добавляем малое значение для избежания деления на 0)\n",
    "        actual_pct = (actual_counts + 0.0001) / len(actual_clean)\n",
    "        expected_pct = (expected_counts + 0.0001) / len(expected_clean)\n",
    "        \n",
    "        # Формула PSI: sum((actual% - expected%) * ln(actual% / expected%))\n",
    "        psi = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))\n",
    "        \n",
    "        return psi\n",
    "    \n",
    "    # Получаем данные актуального периода\n",
    "    actual_period = pdf[pdf[period_col] == actual_period_value]\n",
    "    \n",
    "    if actual_period.empty:\n",
    "        return pd.DataFrame(columns=[period_col, 'variable', 'psi'])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Для каждого expected периода\n",
    "    for period in pdf[period_col].unique():\n",
    "        if period == actual_period_value:\n",
    "            continue\n",
    "            \n",
    "        expected_period = pdf[pdf[period_col] == period]\n",
    "        \n",
    "        # Рассчитываем PSI для каждой переменной\n",
    "        for col in ['value_1', 'value_2', 'value_3']:\n",
    "            psi_value = compute_psi(\n",
    "                actual_period[col], \n",
    "                expected_period[col]\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                period_col: str(period),\n",
    "                'variable': col,\n",
    "                'psi': psi_value\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42cc84b-d61c-4eff-bb7e-c38f6fd05e5e",
   "metadata": {},
   "source": [
    "**Применение функции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cda57c0-39f8-4e5e-b0f0-01ef947d8fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------------+\n",
      "|period_no|variable|                psi|\n",
      "+---------+--------+-------------------+\n",
      "|        1| value_1|0.04462771028534144|\n",
      "|        1| value_2| 0.2043249163152812|\n",
      "|        1| value_3|0.04462771028534144|\n",
      "|        2| value_1| 0.3465551252190449|\n",
      "|        2| value_2|0.07191574652480093|\n",
      "|        2| value_3| 0.3465551252190449|\n",
      "|        3| value_1| 0.7323402152146065|\n",
      "|        3| value_2|  10.30883520747549|\n",
      "|        3| value_3| 0.7323402152146065|\n",
      "+---------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = df_for_psi.groupby().applyInPandas(lambda pdf: calculate_psi(pdf, bins = 1), schema=\"period_no string, variable string, psi double\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb50ba-3582-4d7e-922f-edf32939487d",
   "metadata": {},
   "source": [
    "## PANDAS_UDF.GROUPED_AGG Example\n",
    "\n",
    "PANDAS_UDF.GROUPED_AGG - тип pandas UDF в PySpark, который используется для агрегаций по группам (аналог groupBy().agg(...)), но с возможностью писать агрегирующую функцию на pandas, а не на встроенных функциях Spark.\n",
    "\n",
    "\n",
    "**Особенности GROUPED_MAP UDF:**\n",
    "\n",
    "1. **Работа с Pandas DataFrame** - группа данных передается в функцию как Pandas DataFrame, что позволяет использовать возможности Pandas для обработки данных\n",
    "2. **Обработка по группам** - возможность использования нестандартной агрегирующей функции, которой нет в Spark (медиана, percentiles по особым правилам, кастомный скор, PSI и т.п.).\n",
    "   \n",
    "**Пример определения:**\n",
    "```python\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "# Определяется функция для применения\n",
    "def calculate_fuction(pdf: pd.DataFrame) -> float:\n",
    "    return s.mean()\n",
    "\n",
    "# Применение средствами .applyInPandas\n",
    "dfData.groupby(\"grp\").agg(calculate_fuction(F.col(\"value\")).alias(\"mean_value\"))\n",
    "\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907e47b-3053-43b2-b1e7-a4c467ebdd5d",
   "metadata": {},
   "source": [
    "## 1. Расчет взвешенного среднего"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "196904ef-564d-48d0-95d6-c5fc8140091e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|category|weighted_avg|\n",
      "+--------+------------+\n",
      "|       A|       170.0|\n",
      "|       B|       200.0|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def weighted_average(values: pd.Series, weights: pd.Series) -> float:\n",
    "    return (values * weights).sum() / weights.sum()\n",
    "\n",
    "# Использование\n",
    "df = spark.createDataFrame([\n",
    "    (\"A\", 100, 0.3),\n",
    "    (\"A\", 200, 0.7),\n",
    "    (\"B\", 150, 0.5),\n",
    "    (\"B\", 250, 0.5)\n",
    "], [\"category\", \"value\", \"weight\"])\n",
    "\n",
    "df.groupBy(\"category\").agg(weighted_average(df.value, df.weight).alias(\"weighted_avg\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85092c29-a601-4229-8303-63ade6cb70be",
   "metadata": {},
   "source": [
    "## 2. Расчет перцентилей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01b24621-4490-4432-abd9-0d02f72129b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "| product|p95_price|\n",
      "+--------+---------+\n",
      "|Product1|    290.0|\n",
      "|Product2|    245.0|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def percentile_95(values: pd.Series) -> float:\n",
    "    return values.quantile(0.95)\n",
    "\n",
    "# Использование\n",
    "df = spark.createDataFrame([\n",
    "    (\"Product1\", 100),\n",
    "    (\"Product1\", 200),\n",
    "    (\"Product1\", 300),\n",
    "    (\"Product2\", 150),\n",
    "    (\"Product2\", 250)\n",
    "], [\"product\", \"price\"])\n",
    "\n",
    "df.groupBy(\"product\").agg(percentile_95(df.price).alias(\"p95_price\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e33bf1-44a6-4dee-867a-a5a4683e3259",
   "metadata": {},
   "source": [
    "## 3. Коэффициент вариации (CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bd346b6-1bf6-4f31-903a-c6387534b2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "| store|        cv_percent|\n",
      "+------+------------------+\n",
      "|Store1| 4.761904761904762|\n",
      "|Store2|28.284271247461902|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def coefficient_of_variation(values: pd.Series) -> float:\n",
    "    return (values.std() / values.mean()) * 100 if values.mean() != 0 else 0.0\n",
    "\n",
    "# Использование\n",
    "df = spark.createDataFrame([\n",
    "    (\"Store1\", 100),\n",
    "    (\"Store1\", 110),\n",
    "    (\"Store1\", 105),\n",
    "    (\"Store2\", 200),\n",
    "    (\"Store2\", 300)\n",
    "], [\"store\", \"sales\"])\n",
    "\n",
    "df.groupBy(\"store\").agg(coefficient_of_variation(df.sales).alias(\"cv_percent\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb77d9-e937-4e09-a6db-2bf1d14191ea",
   "metadata": {},
   "source": [
    "## Example cogroup c PANDAS_UDF\n",
    "\n",
    "**cogroup** позволяет обрабатывать две группы данных (два Spark DataFrame) одновременно средствами .applyInPandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd24ba-65bf-49a4-a665-4af29279b20e",
   "metadata": {},
   "source": [
    "### 1. Сравнение данных двух периодов  \n",
    "Например, подсчитет процентного изменения среднего значения фичей между текуцим годом и предыдущим (текуцим годом - 1)\n",
    "\n",
    "$$\n",
    "growth\\_pct = \\frac{\\text{avg}(prev\\_value) - \\text{avg}(last\\_value)}{\\text{avg}(prev\\_value)} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e46dab5-d279-4adc-9f31-0bd46881907d",
   "metadata": {},
   "source": [
    "**Исходные данные**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "478ace26-c4d9-405c-9ff2-abd612e212f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные за два периода\n",
    "df_pr = spark.createDataFrame([\n",
    "    (\"feature1\", \"2024-01\", 0.7), (\"feature1\", \"2024-02\", 3.25), (\"feature1\", \"2024-03\", 0.4),\n",
    "    (\"feature2\", \"2024-01\", 160.0), (\"feature2\", \"2024-02\", 170.0),\n",
    "], [\"feature\", \"period\", \"value\"]) # Значение фичей за предыдущий год\n",
    "\n",
    "df_cr = spark.createDataFrame([\n",
    "    (\"feature1\", \"2024-01\", 1.0), (\"feature1\", \"2024-02\", 2.05),\n",
    "    (\"feature2\", \"2024-01\", 195.0),\n",
    "], [\"feature\", \"period\", \"value\"]) # Значение фичей за текущий год"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2516ab9-1915-4fc7-898b-eabe4dd9cdd2",
   "metadata": {},
   "source": [
    "**Схема результата**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ca8977b-d839-4d07-88c3-7f6bbb9a50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Схема результата\n",
    "schema = StructType([\n",
    "    StructField(\"feature\", StringType(), True),\n",
    "    StructField(\"avg_prev_year\", DoubleType(), True),\n",
    "    StructField(\"avg_last_year\", DoubleType(), True),\n",
    "    StructField(\"growth_pct\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1746ee45-5470-47a4-b016-22f441d509e8",
   "metadata": {},
   "source": [
    "**Функция pandas_udf для применения через cogroup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76770c39-30c6-4c80-937e-56de1d561997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_periods(pdf_prev: pd.DataFrame, pdf2: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Функция сравнения данных двух DataFrame. Определяет процент изменения среднего значения зв период \"\"\"        \n",
    "    if pdf_prev.empty or pdf2.empty:\n",
    "        return pd.DataFrame(columns=[\"feature\", \"avg_prev_year\", \"avg_last_year\", \"growth_pct\"])\n",
    "    \n",
    "    feature = pdf_prev['feature'].iloc[0]\n",
    "    avg_prev_year = pdf_prev['value'].mean()\n",
    "    avg_last_year = pdf2['value'].mean()\n",
    "    growth = ((avg_last_year - avg_prev_year) / avg_prev_year) * 100 if avg_prev_year != 0 else 0\n",
    "    \n",
    "    return pd.DataFrame([[feature, round(avg_prev_year,6), round(avg_last_year,6), round(growth,2)]], \n",
    "                        columns=[\"feature\", \"avg_prev_year\", \"avg_last_year\", \"growth_pct\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62752a33-23a3-4b8d-b620-3d7666f83687",
   "metadata": {},
   "source": [
    "**Применение функции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e84a56c-a9b8-4991-8c55-c346fa293c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+----------+\n",
      "| feature|avg_prev_year|avg_last_year|growth_pct|\n",
      "+--------+-------------+-------------+----------+\n",
      "|feature1|         1.45|        1.525|      5.17|\n",
      "|feature2|        165.0|        195.0|     18.18|\n",
      "+--------+-------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pr.groupBy(\"feature\").cogroup(df_cr.groupBy(\"feature\")).applyInPandas(compare_periods, schema=schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af92d91-bb5c-4a5a-9bfd-69e72c889870",
   "metadata": {},
   "source": [
    "## 2. Расчёт корреляции между данными двух DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9648f73-2912-4cf3-a39d-a31c040bdd4d",
   "metadata": {},
   "source": [
    "**Исходные данные**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac741661-0f3d-44c3-a2e0-93896596ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные: клики и покупки\n",
    "clicks_df = spark.createDataFrame([\n",
    "    (\"User1\", \"2024-01-01\", 10),\n",
    "    (\"User1\", \"2024-01-02\", 15),\n",
    "    (\"User2\", \"2024-01-01\", 5),\n",
    "    (\"User2\", \"2024-01-02\", 15),\n",
    "    (\"User3\", \"2024-01-01\", 2),\n",
    "], [\"user_id\", \"date\", \"clicks\"])\n",
    "\n",
    "purchases_df = spark.createDataFrame([\n",
    "    (\"User1\", \"2024-01-01\", 2),\n",
    "    (\"User1\", \"2024-01-02\", 3),\n",
    "    (\"User2\", \"2024-01-01\", 1),\n",
    "    (\"User2\", \"2024-01-02\", 6),\n",
    "], [\"user_id\", \"date\", \"purchases\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7ca7ce-8500-4173-bc82-6e118234145f",
   "metadata": {},
   "source": [
    "**Схема результата**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31457cd8-a1e5-4937-887f-834f44c0b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"correlation\", DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc14ee-d006-44a5-9708-5d1dcfc388d2",
   "metadata": {},
   "source": [
    "**Функция для применения через .mapInPandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a79dc75-d853-4664-88d2-ee7f3c0a1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation(clicks_pdf: pd.DataFrame, purchases_pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Нормализация данных по партициям (MinMax Scaling) \"\"\"        \n",
    "    if not clicks_pdf.empty:\n",
    "        user_id = clicks_pdf[\"user_id\"].iloc[0]\n",
    "    elif not purchases_pdf.empty:\n",
    "        user_id = purchases_pdf[\"user_id\"].iloc[0]\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"user_id\", \"correlation\"])\n",
    "    \n",
    "    # Объединяем по дате\n",
    "    merged = clicks_pdf.merge(purchases_pdf, on=\"date\", suffixes=(\"_c\", \"_p\"))\n",
    "    \n",
    "    if len(merged) < 2:\n",
    "        return pd.DataFrame([[user_id, np.nan]], columns=[\"user_id\", \"correlation\"])\n",
    "    \n",
    "    user_id = clicks_pdf['user_id'].iloc[0]\n",
    "    corr = merged['clicks'].corr(merged['purchases'])\n",
    "    \n",
    "    return pd.DataFrame([[user_id, corr]], columns=[\"user_id\", \"correlation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5452be-b05b-4f29-8864-5ce3aa8624d2",
   "metadata": {},
   "source": [
    "**Применение функции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a12bc430-ca96-4463-91bf-3660e14a1055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|user_id|       correlation|\n",
      "+-------+------------------+\n",
      "|  User1|0.9999999999999999|\n",
      "|  User2|0.9999999999999999|\n",
      "|  User3|              NULL|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = clicks_df.groupBy(\"user_id\").cogroup( purchases_df.groupBy(\"user_id\")).applyInPandas(calculate_correlation, schema=schema)\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65224508-48cf-4ba9-ad06-341427d04311",
   "metadata": {},
   "source": [
    "## 3. Синхронизация временных рядов (заполнение пропусков значением 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a3332-a9ef-4cc7-a41a-218f78856804",
   "metadata": {},
   "source": [
    "**Исходные данные**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95ec50af-65f9-47cf-a040-8305690d9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Плановые и фактические данные\n",
    "planned_df = spark.createDataFrame([\n",
    "    (\"Task1\", \"2024-01-01\", 1000),\n",
    "    (\"Task1\", \"2024-01-03\", 1200),\n",
    "], [\"task\", \"date\", \"planned_value\"])\n",
    "\n",
    "actual_df = spark.createDataFrame([\n",
    "    (\"Task1\", \"2024-01-01\", 950),\n",
    "    (\"Task1\", \"2024-01-02\", 1050),\n",
    "], [\"task\", \"date\", \"actual_value\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382fd366-cb82-4d24-a6cd-d9857ded77e8",
   "metadata": {},
   "source": [
    "**Схема результата**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ae9e912-0332-44b7-98c6-377a81194c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"task\", StringType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"planned_value\", DoubleType()),\n",
    "    StructField(\"actual_value\", DoubleType()),\n",
    "    StructField(\"variance\", DoubleType())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90213e-47b4-4580-8ac6-2464fca9057b",
   "metadata": {},
   "source": [
    "**Функция для применения**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "895a7254-08dd-4d2d-a80b-008ed900174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_fill(planned_pdf: pd.DataFrame, actual_pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    if planned_pdf.empty and actual_pdf.empty:\n",
    "        return pd.DataFrame(columns=schema.fieldNames())\n",
    "    \n",
    "    task = planned_pdf['task'].iloc[0] if not planned_pdf.empty else actual_pdf['task'].iloc[0]\n",
    "    \n",
    "    # Объединяем с заполнением пропусков\n",
    "    merged = pd.merge(planned_pdf, actual_pdf, on=\"date\", how=\"outer\", suffixes=(\"_p\", \"_a\"))\n",
    "    merged['task'] = task\n",
    "    merged['planned_value'] = merged['planned_value'].fillna(0)\n",
    "    merged['actual_value'] = merged['actual_value'].fillna(0)\n",
    "    merged['variance'] = merged['actual_value'] - merged['planned_value']\n",
    "    \n",
    "    return merged[['task', 'date', 'planned_value', 'actual_value', 'variance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae74f6-e383-4fb9-8d8f-fce29485e1e6",
   "metadata": {},
   "source": [
    "**Применение через applyInPandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e07bed7-21c8-4170-b9a6-c3886cc68b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------+------------+--------+\n",
      "| task|      date|planned_value|actual_value|variance|\n",
      "+-----+----------+-------------+------------+--------+\n",
      "|Task1|2024-01-01|       1000.0|       950.0|   -50.0|\n",
      "|Task1|2024-01-02|          0.0|      1050.0|  1050.0|\n",
      "|Task1|2024-01-03|       1200.0|         0.0| -1200.0|\n",
      "+-----+----------+-------------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = planned_df.groupBy(\"task\").cogroup(actual_df.groupBy(\"task\")).applyInPandas(merge_and_fill, schema=schema)\n",
    "\n",
    "result.orderBy(\"date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4586ea7d-e5a4-435e-8922-89f1f8449536",
   "metadata": {},
   "source": [
    "## 4. A/B тест: сравнение контрольной и тестовой групп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8af9060c-ab15-4856-9f8c-607a526705c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+------------------+--------------------+\n",
      "| campaign|control_avg|test_avg|        uplift_pct|             p_value|\n",
      "+---------+-----------+--------+------------------+--------------------+\n",
      "|Campaign1|      105.0|   125.0|19.047619047619047|0.008049893100837717|\n",
      "+---------+-----------+--------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Контрольная группа\n",
    "control_df = spark.createDataFrame([\n",
    "    (\"Campaign1\", 1, 100),\n",
    "    (\"Campaign1\", 2, 110),\n",
    "    (\"Campaign1\", 3, 105),\n",
    "], [\"campaign\", \"user_id\", \"revenue\"])\n",
    "\n",
    "# Тестовая группа\n",
    "test_df = spark.createDataFrame([\n",
    "    (\"Campaign1\", 4, 120),\n",
    "    (\"Campaign1\", 5, 130),\n",
    "    (\"Campaign1\", 6, 125),\n",
    "], [\"campaign\", \"user_id\", \"revenue\"])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"campaign\", StringType()),\n",
    "    StructField(\"control_avg\", DoubleType()),\n",
    "    StructField(\"test_avg\", DoubleType()),\n",
    "    StructField(\"uplift_pct\", DoubleType()),\n",
    "    StructField(\"p_value\", DoubleType())\n",
    "])\n",
    "\n",
    "def ab_test_analysis(control_pdf: pd.DataFrame, test_pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    from scipy import stats\n",
    "    \n",
    "    if control_pdf.empty or test_pdf.empty:\n",
    "        return pd.DataFrame(columns=schema.fieldNames())\n",
    "    \n",
    "    campaign = control_pdf['campaign'].iloc[0]\n",
    "    control_avg = control_pdf['revenue'].mean()\n",
    "    test_avg = test_pdf['revenue'].mean()\n",
    "    uplift = ((test_avg - control_avg) / control_avg) * 100 if control_avg != 0 else 0\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, p_value = stats.ttest_ind(control_pdf['revenue'], test_pdf['revenue'])\n",
    "    \n",
    "    return pd.DataFrame([[campaign, control_avg, test_avg, uplift, p_value]], \n",
    "                        columns=schema.fieldNames())\n",
    "\n",
    "result = control_df.groupBy(\"campaign\").cogroup(\n",
    "    test_df.groupBy(\"campaign\")\n",
    ").applyInPandas(ab_test_analysis, schema=schema)\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b029c78e-8d27-4667-8f37-eba06b8f0158",
   "metadata": {},
   "source": [
    "## Example PANDAS_UDF с применением срествами .mapInPandas\n",
    "\n",
    "mapInPandas - это метод Spark DataFrame, позволящий обрабатывать данные партициями в виде pandas.DataFrame и возвращать pandas.DataFrame. (pandas‑аналог mapPartitions)\n",
    "\n",
    "**Особенности**\n",
    "\n",
    "1. **Обработка по партициям Spark** - Итериратор по партициям Spark DataFrame и yield‑итератор pandas.DataFrame как возвращаемое значение\n",
    "2. **Схема вывода** - необходимо явно определять схему вывода с помощью StructType\n",
    "3. **Векторизовация операций** - операции производятся над целыми колонками данных, что значительно ускоряет их выполнение по сравнению с классическими циклическими подходами\n",
    "\n",
    "```python\n",
    "DataFrame.mapInPandas(func: PandasMapIterFunction\n",
    "                     ,schema: Union[pyspark.sql.types.StructType, str]\n",
    "                    , barrier: bool = False) \n",
    "return DataFrame\n",
    "```   \n",
    "\n",
    "**Пример определения:**\n",
    "```python\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "# Определяется схема вывода\n",
    "output_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True)\n",
    "                    ])\n",
    "\n",
    "# Определяется функция для применения\n",
    "def calculate_fuction(pdf: pd.DataFrame):\n",
    "    for pdf in iterator:\n",
    "        yield pdf[pdf.id == 1]\n",
    "\n",
    "# Прменение средствами .applyInPandas\n",
    "df.mapInPandas(calculate_fuction, schema=output_schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31968d-1366-4df6-80d4-1ea5d8205a0b",
   "metadata": {},
   "source": [
    "### 1. Нормализация данных по партициям (MinMax Scaling)\n",
    "\n",
    "Min–Max нормализация - масштабирование в диапазон (0...1)\n",
    "\n",
    "Для каждого признака расчитывается внутри группы:\n",
    "\n",
    "$ \n",
    "x_{\\text{norm}} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1012ca19-71bd-4808-9b91-4c1bacc9e9b2",
   "metadata": {},
   "source": [
    "**Исходные данные**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7724aeca-c0ee-475d-a6ff-1e3fc109672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"Product1\", 50),  (\"Product1\", 100), (\"Product1\", 150), (\"Product1\", 200),\n",
    "    (\"Product2\", 150), (\"Product2\", 160), (\"Product2\", 300),\n",
    "    (\"Product3\", 60),\n",
    "], [\"product\", \"sales\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25390bd9-da91-4a05-8aef-f68c1ebcd47c",
   "metadata": {},
   "source": [
    "**Схема результата**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "746ccd80-212d-4e70-89d7-bc8a1e18527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"product\", StringType()),\n",
    "    StructField(\"sales\", DoubleType()),\n",
    "    StructField(\"normalized_sales\", DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a93c8-dfb9-4d7a-b202-4eac1f95f948",
   "metadata": {},
   "source": [
    "**Функция для применения через .mapInPandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1a6a8b18-0228-418d-8761-197aab8242dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_partition(iterator):\n",
    "    \"\"\" Нормализация данных по партициям (MinMax Scaling) \"\"\"    \n",
    "    for pdf in iterator:\n",
    "        if pdf.empty:\n",
    "            yield pdf\n",
    "            continue\n",
    "        \n",
    "        # MinMax нормализация внутри партиции\n",
    "        min_val = pdf['sales'].min()\n",
    "        max_val = pdf['sales'].max()\n",
    "        \n",
    "        if max_val - min_val != 0:\n",
    "            pdf['normalized_sales'] = (pdf['sales'] - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            pdf['normalized_sales'] = 0.0\n",
    "        \n",
    "        yield pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc6cca-d139-4b1c-a1fd-0ec98a8cbd87",
   "metadata": {},
   "source": [
    "**Применение функции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a3cfaee-eacf-4751-a04f-78f980ccb74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------------+\n",
      "| product|sales|normalized_sales|\n",
      "+--------+-----+----------------+\n",
      "|Product1| 50.0|             0.0|\n",
      "|Product1|100.0|             0.2|\n",
      "|Product1|150.0|             0.4|\n",
      "|Product1|200.0|             0.6|\n",
      "|Product2|150.0|             0.4|\n",
      "|Product2|160.0|            0.44|\n",
      "|Product2|300.0|             1.0|\n",
      "|Product3| 60.0|            0.04|\n",
      "+--------+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.repartition(1).mapInPandas(normalize_partition, schema=schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6489cebf-c83b-4d73-9658-f802d98bf3b4",
   "metadata": {},
   "source": [
    "### 2. Обработка временных рядов с интерполяцией\n",
    "\n",
    "Интерполяция пропущенных значений — это способ восстановления недостающих данных по уже известным соседним точкам. Предполагается что значение меняется плавно.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f567b47a-704e-4b15-8123-0ef88967b326",
   "metadata": {},
   "source": [
    "**Исходные данные с пропусками**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8a444ee0-8d0a-4149-82a1-79f1164d3158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"Sensor1\", \"2024-01-01\", 20.5),\n",
    "    (\"Sensor1\", \"2024-01-03\", 22.0),  # Пропущен 2024-01-02\n",
    "    (\"Sensor1\", \"2024-01-04\", 21.5),\n",
    "    (\"Sensor2\", \"2024-01-01\", 18.0),\n",
    "    (\"Sensor2\", \"2024-01-04\", 19.5),  # Пропущены 02 и 03\n",
    "], [\"sensor_id\", \"date\", \"temperature\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a04283-f0fe-41c5-9e22-1ef0bcc82a04",
   "metadata": {},
   "source": [
    "**Схема результата**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a638d5c5-8ba1-44ea-94aa-e4ebe0bc01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"sensor_id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"is_interpolated\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afbd86c-952f-4bc3-bbaa-b7e84d7586ba",
   "metadata": {},
   "source": [
    "**Функция для применения через .mapInPandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a40d4e58-9d97-47e9-9207-ae6fe5bd7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Линейная интерполяция\n",
    "def interpolate_timeseries(iterator):\n",
    "    \"\"\"\n",
    "    Линейная интерполяция пропущенных значений в временных рядах.\n",
    "    Обрабатывает каждую партицию отдельно.\n",
    "    \"\"\"\n",
    "    for pdf in iterator:\n",
    "        if pdf.empty:\n",
    "            yield pdf\n",
    "            continue\n",
    "        \n",
    "        # Преобразуем строки в datetime\n",
    "        pdf['date'] = pd.to_datetime(pdf['date'])\n",
    "        pdf = pdf.sort_values(['sensor_id', 'date'])\n",
    "        \n",
    "        result_dfs = []\n",
    "        \n",
    "        # Для каждого сенсора отдельно создаётся полный диапазон дат\n",
    "        for sensor_id, group in pdf.groupby('sensor_id'):\n",
    "            date_range = pd.date_range(start=group['date'].min(), end=group['date'].max(), freq='D')\n",
    "            \n",
    "            # DataFrame с полным диапазоном дат\n",
    "            full_dates = pd.DataFrame({'date': date_range,'sensor_id': sensor_id})\n",
    "            \n",
    "            # Объединение с исходными данными\n",
    "            merged = full_dates.merge(group[['date', 'temperature']], on='date', how='left')\n",
    "            \n",
    "            # Метка интерполированных значений (Yes)\n",
    "            merged['is_interpolated'] = merged['temperature'].isna().map({True: 'Yes', False: 'No'})\n",
    "            \n",
    "            # Применение линейной интерполяции\n",
    "            merged['temperature'] = merged['temperature'].interpolate(method='linear')\n",
    "            \n",
    "            result_dfs.append(merged)\n",
    "        \n",
    "        # Объединяем все сенсоры\n",
    "        if result_dfs:\n",
    "            result = pd.concat(result_dfs, ignore_index=True)\n",
    "            result['date'] = result['date'].dt.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Итератор в правильном порядке колонок\n",
    "            yield result[['sensor_id', 'date', 'temperature', 'is_interpolated']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0376a06a-e430-43a6-8b2a-553993e82201",
   "metadata": {},
   "source": [
    "**Применение функции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "16568f7b-1de6-41c3-a174-678b81ecb161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+---------------+\n",
      "|sensor_id|      date|temperature|is_interpolated|\n",
      "+---------+----------+-----------+---------------+\n",
      "|  Sensor1|2024-01-01|       20.5|             No|\n",
      "|  Sensor1|2024-01-03|       22.0|             No|\n",
      "|  Sensor1|2024-01-04|       21.5|             No|\n",
      "|  Sensor2|2024-01-01|       18.0|             No|\n",
      "|  Sensor2|2024-01-02|       18.5|            Yes|\n",
      "|  Sensor2|2024-01-03|       19.0|            Yes|\n",
      "|  Sensor2|2024-01-04|       19.5|             No|\n",
      "+---------+----------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.mapInPandas(interpolate_timeseries, schema=schema).orderBy(\"sensor_id\", \"date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28defd77-dc81-4e3d-8082-5780f589c6d0",
   "metadata": {},
   "source": [
    "# Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "64ddfa3d-cad7-46d9-9018-64d8bc8a5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45185c0-44a0-4e5e-96be-47a1dfe1f183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
