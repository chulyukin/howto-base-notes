{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d7ab3e8-a4ed-488d-bebc-2d301774bc5d",
   "metadata": {},
   "source": [
    "# Spark 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a18c73-d947-4fe4-b15a-cefcb91390ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/29 01:38:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"python\")\n",
    "os.environ[\"SPARK_LOCAL_IP\"]='localhost'\n",
    "from pyspark import SparkContext, SparkConf#, HiveContext\n",
    "conf = SparkConf()\\\n",
    "             .setAppName(\"Example Spark\")\\\n",
    "             .setMaster(\"local[2]\")\\\n",
    "             .setAppName(\"CountingSheep\")\\\n",
    "             .set(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4330b6e6-aa4c-4f9d-89c2-85a63f76fa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.6\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.13.3 (main, Jun 16 2025 18:15:32)\n",
      "Spark context Web UI available at http://localhost:4040\n",
      "Spark context available as 'sc' (master = local[2], app id = local-1751150319437).\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8b8d3e4-8bee-4b7b-98c2-59791ac98c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CountingSheep</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x780df0761d30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "707fe64d-03fc-4d44-bcb9-b019892126fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f_\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d738ce-403b-486b-b729-f29bbcf7e80d",
   "metadata": {},
   "source": [
    "# Example DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e4c6c26-51c1-4415-9f5a-d58827293a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(rowno=1, double_value=3., str_value='Value 1', date_value=date(2004, 5, 9), timestamp_value=datetime(2002, 1, 1, 12, 0)),\n",
    "    Row(rowno=2, double_value=4., str_value='Value 2', date_value=date(2004, 5, 2), timestamp_value=datetime(2002, 1, 2, 15, 15)),\n",
    "    Row(rowno=3, double_value=5., str_value='Value 3', date_value=date(2007, 3, 1), timestamp_value=datetime(2002, 1, 3, 19, 23))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4131f62-abc7-4308-8907-ab59cfb03766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------+----------+-------------------+\n",
      "|rowno|double_value|str_value|date_value|    timestamp_value|\n",
      "+-----+------------+---------+----------+-------------------+\n",
      "|    1|         3.0|  Value 1|2004-05-09|2002-01-01 12:00:00|\n",
      "|    2|         4.0|  Value 2|2004-05-02|2002-01-02 15:15:00|\n",
      "|    3|         5.0|  Value 3|2007-03-01|2002-01-03 19:23:00|\n",
      "+-----+------------+---------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cccf8f49-6d1c-4f25-adf4-de21c4c7e91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rowno', 'bigint'),\n",
       " ('double_value', 'double'),\n",
       " ('str_value', 'string'),\n",
       " ('date_value', 'date'),\n",
       " ('timestamp_value', 'timestamp')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d0dce-75df-4278-84c3-4560ca4f1a13",
   "metadata": {},
   "source": [
    "# Local DataBases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26181d23-bec0-434b-af29-933fe35f5ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop database if exists my_db cascade\")\n",
    "spark.sql(\"create database my_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b584d8b-399f-4f88-9119-b09c12c95a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|    my_db|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b36e03-d098-4fe6-b004-7d1b07ba9710",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d61f08c-c890-49f6-b847-c907defff77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"default.my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7b25539-2605-48a7-8c4f-04cfafd85a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"my_db.new_my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9abcfd1-291a-4eed-90cc-a0208d34548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.table(\"default.my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fefc4bd6-6db7-4ba4-88a8-b4fbcfff7734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------+----------+-------------------+\n",
      "|rowno|double_value|str_value|date_value|    timestamp_value|\n",
      "+-----+------------+---------+----------+-------------------+\n",
      "|    1|         3.0|  Value 1|2004-05-09|2002-01-01 12:00:00|\n",
      "|    2|         4.0|  Value 2|2004-05-02|2002-01-02 15:15:00|\n",
      "|    3|         5.0|  Value 3|2007-03-01|2002-01-03 19:23:00|\n",
      "+-----+------------+---------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad4998c0-410b-4bc8-a476-9f1e98397d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.table(\"my_db.new_my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "141c7f44-b058-4407-9aa9-2a170ea19d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------+----------+-------------------+\n",
      "|rowno|double_value|str_value|date_value|    timestamp_value|\n",
      "+-----+------------+---------+----------+-------------------+\n",
      "|    1|         3.0|  Value 1|2004-05-09|2002-01-01 12:00:00|\n",
      "|    2|         4.0|  Value 2|2004-05-02|2002-01-02 15:15:00|\n",
      "|    3|         5.0|  Value 3|2007-03-01|2002-01-03 19:23:00|\n",
      "+-----+------------+---------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "519532dc-76b4-4232-8fa2-9fb0c5a68ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default| my_table|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables from default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e9abf37-76d2-41e6-94e3-ebd3f77410df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.table(\"my_db.new_my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b26b1e5e-1e3d-4481-bbca-63177ce8cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------+----------+-------------------+\n",
      "|rowno|double_value|str_value|date_value|    timestamp_value|\n",
      "+-----+------------+---------+----------+-------------------+\n",
      "|    1|         3.0|  Value 1|2004-05-09|2002-01-01 12:00:00|\n",
      "+-----+------------+---------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.where(f_.col(\"rowno\")==1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65e80ae6-6323-4197-adad-30d1257bb6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|    my_db|new_my_table|      false|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables from my_db\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64ddfa3d-cad7-46d9-9018-64d8bc8a5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948806e-3cc9-4b85-b718-3d017b12455e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
